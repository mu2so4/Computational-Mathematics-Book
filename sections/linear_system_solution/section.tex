\documentclass{article}

\begin{document}
\section{Вычислительные методы линейной алгебры}
Предмет исследования данного раздела -- это система линейных алгебраических
уравнений (СЛАУ)
\[
	\begin{cases}
		a_{11}x_1+a_{12}x_2+...+a_{1n}x_n=f_1, \\
		a_{21}x_1+a_{22}x_2+...+a_{2n}x_n=f_2, \\
		... \\
		a_{n1}x_1+a_{n2}x_2+...+a_{nn}x_n=f_n. \\
	\end{cases}
\]

Также можно записать систему в матричной форме как
\[Ax=f.\]

Если определитель матрицы $A$ отличен от нуля, то решение существует, и притом
только одно. Казалось, почему нельзя просто применить для решения метод Крамера
или обращение матрицы? Как и всегда: трудоёмкость этих методов быстро растёт с
ростом количества уравнений системы.

Например, для решения СЛАУ методом Крамера необходимо посчитать $n+1$
определитель матрицы, а вычислительная сложность этой операции составляет
$O(n!)$. В итоге, общая вычислительная мощность составляет примерно $O((n+1)!)$!
Для понимания масштабов трагедии: если система состоит из 20 уравнений, на
одну атомарную операцию уходит $10^{-12}$ секунд, и вычисления не
распараллеливаются и не векторизуются, то для решения потребуется 1.62 года
непрерывных вычислений, а также бесперебойной подачи электроэнергии.

А количество уравнений в системе можеть быть гораздо больше двадцати. Поэтому
большие системы решают иначе. В этом разделе будут рассмотрены методы Гаусса и
итерационные методы.

\newpage

\subsection{Линейные пространства}
Прежде чем как начать разбирать методы решения систем линейных уравнений,
освежим знания из линейной алгебры.

\begin{define}
	Пространство $L$ над полем $K$, в котором определены операции сложения
	$+: V^2\rightarrow V$ и умножения $*: K\times V\rightarrow V$,
	называется \textbf{линейным пространством}, если в нём верны следующие
	аксиомы:

	\begin{enumerate}[nosep]
		\item $\forall u,v\in V: u+v=u+v$ -- коммутативность сложения,
		\item $\forall u,v,w\in V: (u+v)+w=u+(v+w)$ -- ассоциативность
			сложения,
		\item $\exists\overline{0}: \forall v\in V:
			v+\overline{0}=\overline{0}+v=v$ -- существование
			нейтрального по сложению элемента,
		\item $\forall v\in V\;\exists (-v): v+(-v)=(-v)+v=\overline{0}$
			-- существование противоположного элемента,
		\item $\forall\lambda\in K,\;\forall u,v\in V: \lambda(u+v)=
			\lambda u+\lambda v$ -- дистрибутивность умножения
			относительно сложения векторов,
		\item $\forall\lambda,\mu\in K,\;\forall v\in V:
			(\lambda+\mu)v=\lambda v+\mu v$ -- дистрибутивность
			умножения,
		\item $\forall\lambda,\mu\in K,\;\forall v\in V: (\lambda\mu)v=
			\lambda(\mu v)$ -- ассоциативность умножения,
		\item $\exists 1\in K: \forall v\in V: 1\cdot v=v$ --
			существование нейтрального по умножению элемента.
	\end{enumerate}
\end{define}

При поиске решения СЛАУ полагаются $K=\mathbb R$ и $V=\mathbb R^n$.

\begin{define}\label{eq:dot_production}
	\textbf{Скалярным произведением} векторов из линейного пространства
	$\mathbb R^n$ называется отображение $\cdot: \mathbb R^n\times
	\mathbb R^n\rightarrow \mathbb R$, удовлетворяющее следующим условиям:
	\begin{enumerate}[nosep]
		\item $\forall v\in R^n: (v,v)\ge 0$, причём $(v,v)=0
			\Leftrightarrow v=\overline{0}$ -- положительная
			определённость;
		\item $\forall u,v\in \mathbb R^n: (u,v)=(v,u)$ --
			коммутативность;
		\item $\forall \lambda,\mu \in\mathbb R^n,\;
			\forall u,v,w\in \mathbb R^n: (\lambda u+\mu v, w)=
			\lambda(u,w)+\mu(v,w)$ -- линейность.
	\end{enumerate}
\end{define}
\newpage

Тут специально было приведено определение для вещественных векторов, потому что
над полем комплекных чисел коммутативность заменяется на сопряжённость. При
работе со СЛАУ векторы вещественные.

\begin{define}\label{eq:vector_norm}
	\textbf{Нормой вектора} из линейного пространства $V$ над полем $K$
	называется отображение $\|\|: V\rightarrow K$, удовлетворяющее
	следующим условиям:
	\begin{enumerate}[nosep]
		\item $\forall v\in V: \|v\|\ge 0$, причём $\|v\|=0
			\Leftrightarrow v=\overline{0}$;
		\item $\forall \lambda\in K,\;\forall v\in V:
			\|\lambda v\|=|\lambda|\cdot\|v\|$;
		\item $\forall u,v\in V: \|u+v\|\le\|u\|+\|v\|$ -- неравенство
			треугольника.
	\end{enumerate}
\end{define}

\begin{define}\label{eq:some_vector_norms}
	Определим следующие нормы над линейным пространством $\mathbb R^n$:
	\begin{enumerate}[nosep]
		\item $\mathlarger{\|x\|_2=\sqrt{(x,x)_2}=\sqrt{\sum_{i=1}^{n}
			x_i^2}}$ -- \textbf{Евклидова норма};
		\item $\mathlarger{\|x\|_1=\sum_{i=1}^{n}|x_i|}$ --
			\textbf{октаэдрическая норма}; \\
		\item $\mathlarger{\|x\|_\infty=\max_{i\in\overline{1,n}}
			|x_i|}$ -- \textbf{кубическая норма}.
	\end{enumerate}
\end{define}

Несложно проверить, что это в действительности нормы.

\subsubsection{Ортонормированные базисы}
\begin{define}
	\textbf{Базис} -- максимальный по включению упорядоченный набор линейно
	независимых векторов линейного пространства $V$.
\end{define}

Напомним важное свойство базиса пространства.
\begin{theorem}
	Всякий вектор $v$ из линейного пространства $V$ представим в виде
	линейной комбинации базисных векторов данного пространства, и притом
	единственным образом.
\end{theorem}

\begin{define}
	\textbf{Ортонормированный базис} линейного пространства $V$ над полем
	$K$ -- базис $e_1,...,e_n$ такой, что
	\begin{enumerate}[nosep]
		\item $\forall i,j\in\overline{1,k}: (e_i,e_j)=\delta_{ij}$.
	\end{enumerate}
\end{define}

\subsubsection{Матрицы}
\begin{define}
	Матрица $A$ \textbf{положительно определённая} над линейным
	пространством $\mathbb R^n$, если для всякого ненулевого вектора
	$v\in \mathbb R^n$ верно $(Av, v)_2>0$.
\end{define}

\begin{define}
	\textbf{Норма матрицы} $\mathbb R^{n\times m}$
	называется отображение $\|\|: \mathbb R^{n\times m}\rightarrow
	\mathbb R$, удовлетворяющее следующим условиям:
	\begin{enumerate}[nosep]
		\item $\forall A\in\mathbb R^{n\times m}: \|A\|\ge 0$, причём
			$\|A\|=0\Leftrightarrow A=O$;
		\item $\forall \lambda\in \mathbb R,\;\forall v\in
			\mathbb R^{n\times m}:
			\|\lambda A\|=|\lambda|\cdot\|A\|$;
		\item $\forall A,B\in\mathbb R^{n\times m}: \|A+B\|\le\|A\|+\|B\|$ --
			неравенство треугольника.
	\end{enumerate}
\end{define}

\begin{define}
	Норма $\|A\|$ матрицы $A\in \mathbb R^{n\times n}$ называется
	\textbf{нормой, подчинённой векторной норме} $\|x\|$
	\eqref{eq:vector_norm}, где $x\in \mathbb R^n$, если верно
	\[\|A\|=\max_{x\ne \overline{0}}\frac{\|Ax\|}{\|x\|}.\]
\end{define}

\begin{lemma}\label{eq:subordinate_norm_properties}
	Для всякой матричной нормы $\|A\|$, где $A\in\mathbb R^{n\times n}$,
	которая подчинена векторной норме $\|x\|$, где $x\in\mathbb R^n$, верны
	следующие неравенства:
	\begin{enumerate}[nosep]
		\item $\forall A\in\mathbb R^{n\times n},\;\forall x\in\mathbb
			R^n: \|Ax\|\le\|A\|\|x\|$;
		\item $\forall A,B\in\mathbb R^{n\times n}:
			\|AB\|\le\|A\|\|B\|$.
	\end{enumerate}
\end{lemma}

\begin{proof}
	Первое неравенство сразу доказывается из определения подчинённой нормы.

	Второе доказывается чуть дольше:
	\[\|AB\|=\max_{x\ne\overline{0}}\frac{\|A(Bx)\|}{\|x\|}\le
	\max_{x\ne\overline{0}}\frac{\|A\|\|Bx\|}{\|x\|}=\|A\|\|B\|.\]
\end{proof}

\begin{lemma}
	Явные формы матричных норм, подчинённых октаэдрической и кубическим
	нормам \eqref{eq:some_vector_norms}, имеют вид
	\[\|A\|_1=\max_{j\in\overline{1,n}}\sum_{i=1}^{n}|a_{ij}|,\qquad
	\|A\|_\infty=\max_{i\in\overline{1,n}}\sum_{j=1}^{n}|a_{ij}|.\]
\end{lemma}

\proofexercise

В этом преимущество этих норм: подчинённые им матричные нормы вычисляются
непосредственно по элементам матрицы $A$, чего не скажешь о Евклидовой норме.

\begin{define}
	Скаляр $\lambda$ и ненулевой вектор $v$ называются \textbf{собственным
	значением} и \textbf{собственным вектором} матрицы $A$ соответственно,
	если для них выполняется равенство
	\[Av=\lambda v.\]
\end{define}

У собственных значений симметричных матриц есть одно замечательное свойство.
\begin{lemma}
	Если матрица $A$ симметричная, то все её собственные значения
	действительные.
\end{lemma}

\begin{theorem}[о Евклидовой норме симметричной матрицы]
\label{eq:symmetric_matrix_norm}
	Если матрица $A$ симметричная, то её норма равна $\|A\|_2=
	\max|\lambda(A)|$, где $\lambda(A)$ -- спектр матрицы, или же множество
	её собственных значений.
\end{theorem}
\newpage

\subsection{Система линейных алгебраических уравнений}
К решению систем линейных алгебраических уравнений сводятся практически все
задачи вычислительной математики.

Прежде чем переходить к изложению собственно методов, проведём краткое
исследование основных свойств самой системы линейных алгебраических уравнений. В
частности, чувствительности её решения к малым изменениям входных данных.

\begin{define}
	Будем считать, что задача поставлена \textbf{корректно}, если:
	\begin{enumerate}[nosep]
		\item Решение $x$ существует;
		\item Решение $x$ единственно;
		\item Решение $x$ непрерывно зависит от входных данных.
	\end{enumerate}
\end{define}

Если первые 2 пункта выполняются при $det(A)\ne 0$, так как тогда существует
обратная матрица, то третий пункт нужно рассмотреть поподробней.

\begin{theorem}[об оценке погрешности решения СЛАУ]
	Если $\delta f$ -- погрешность $f$ из системы линейных алгебраических
	уравнений $Ax=f$, матрица $A$ невырожденная, то погрешность решения
	$\delta x$ имеет оценку
	\[\boxed{\frac{\|\delta x\|}{\|x\|}\le\nu(A)\frac{\|\delta f\|}
	{\|f\|}},\]
	где $\nu(A)=\|A\|\|A^{-1}\|$ -- \textbf{число обусловленности}.
\end{theorem}

\begin{proof}
	Рассмотрим следующее равенство:
	\[A\delta x=\delta f.\]
	Поскольку матрица невырожденная, её можно обратить и перенести во вторую
	половину равенства:
	\[\delta x=A^{-1}\delta f.\]

	Равенство значений означает равенство норм. Воспользуемся оценкой по
	лемме \eqref{eq:subordinate_norm_properties} и получим
	\[\|\delta x\|\le\|A^{-1}\|\|\delta f\|.\]

	К исходному равенству $Ax=f$ применяем ту же оценку:
	\[\|f\|\le\|A\|\|x\|.\]

	Перемножим эти неравенства:
	\[\|\delta x\|\|f\|\le\|A\|\|A^{-1}\|\|x\|\|\delta f\|.\]

	Из него и следует оценка.
\end{proof}

Если $\nu(A)$ велико, то лишь очень малые погрешности входных данных гарантируют
малую погрешность решения. В связи с этим, про матрицы, у которых $\nu(A)$
велико, говорят, что они плохо обусловлены, а матрицы с относительно малой
величиной $\nu(A)$ называют хорошо обусловленными.

\begin{example}
	Рассмотрим матрицу
	$A=
		\begin{pmatrix}
			1	& 1 \\
			1	& 1+10^{-4} \\
		\end{pmatrix}
	$
	и систему линейных уравнений с ней
	\[
		\begin{pmatrix}
			1	& 1 \\
			1	& 1+10^{-4} \\
		\end{pmatrix}
		\begin{pmatrix}
			2 \\
			0 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			2 \\
			2 \\
		\end{pmatrix}
		.
	\]

	Теперь мы испортим систему, немного изменив $f$:
	\[f+\delta f=
		\begin{pmatrix}
			2 \\
			2+10^{-4} \\
		\end{pmatrix}
		\quad\Leftrightarrow\quad
		x+\delta x=
		\begin{pmatrix}
			1 \\
			1 \\
		\end{pmatrix}
	\]

	Совсем небольшое изменение условия привело к огромному изменению
	решения системы. Такую систему решать нельзя.

	Теперь найдём обусловленность матрицы $A$ по Евклидовой норме.
	Воспользуемся тем, что матрица симметричная
	\eqref{eq:symmetric_matrix_norm}, тогда $\|A\|_2\approx 2$.
	Норма же обратной матрицы, которая тоже симметричная,
	$\|A^{-1}\|_2\approx 2\cdot 10^4$. И тогда $\nu(A)\approx
	4\cdot 10^4$, то есть матрица плохо обусловленная. Результат был
	ожидаем.
\end{example}

\end{document}
