\documentclass{article}

\begin{document}
\section{Вычислительные методы линейной алгебры}
Предмет исследования данного раздела -- это система линейных алгебраических
уравнений (СЛАУ)
\[
	\begin{cases}
		a_{11}x_1+a_{12}x_2+...+a_{1n}x_n=f_1, \\
		a_{21}x_1+a_{22}x_2+...+a_{2n}x_n=f_2, \\
		... \\
		a_{n1}x_1+a_{n2}x_2+...+a_{nn}x_n=f_n. \\
	\end{cases}
\]

Также можно записать систему в матричной форме как
\[Ax=f.\]

Если определитель матрицы $A$ отличен от нуля, то решение существует, и притом
только одно. Казалось, почему нельзя просто применить для решения метод Крамера
или обращение матрицы? Как и всегда: трудоёмкость этих методов быстро растёт с
ростом количества уравнений системы.

Например, для решения СЛАУ методом Крамера необходимо посчитать $n+1$
определитель матрицы, а вычислительная сложность этой операции составляет
$O(n!)$. В итоге, общая вычислительная мощность составляет примерно $O((n+1)!)$!
Для понимания масштабов трагедии: если система состоит из 20 уравнений, на
одну атомарную операцию уходит $10^{-12}$ секунд, и вычисления не
распараллеливаются и не векторизуются, то для решения потребуется 1.62 года
непрерывных вычислений, а также бесперебойной подачи электроэнергии.

А количество уравнений в системе можеть быть гораздо больше двадцати. Поэтому
большие системы решают иначе. В этом разделе будут рассмотрены методы Гаусса и
итерационные методы.

\newpage

\subsection{Линейные пространства}
Прежде чем как начать разбирать методы решения систем линейных уравнений,
освежим знания из линейной алгебры.

\begin{define}
	Пространство $L$ над полем $K$, в котором определены операции сложения
	$+: V^2\rightarrow V$ и умножения $*: K\times V\rightarrow V$,
	называется \textbf{линейным пространством}, если в нём верны следующие
	аксиомы:

	\begin{enumerate}[nosep]
		\item $\forall u,v\in V: u+v=u+v$ -- коммутативность сложения,
		\item $\forall u,v,w\in V: (u+v)+w=u+(v+w)$ -- ассоциативность
			сложения,
		\item $\exists\overline{0}: \forall v\in V:
			v+\overline{0}=\overline{0}+v=v$ -- существование
			нейтрального по сложению элемента,
		\item $\forall v\in V\;\exists (-v): v+(-v)=(-v)+v=\overline{0}$
			-- существование противоположного элемента,
		\item $\forall\lambda\in K,\;\forall u,v\in V: \lambda(u+v)=
			\lambda u+\lambda v$ -- дистрибутивность умножения
			относительно сложения векторов,
		\item $\forall\lambda,\mu\in K,\;\forall v\in V:
			(\lambda+\mu)v=\lambda v+\mu v$ -- дистрибутивность
			умножения,
		\item $\forall\lambda,\mu\in K,\;\forall v\in V: (\lambda\mu)v=
			\lambda(\mu v)$ -- ассоциативность умножения,
		\item $\exists 1\in K: \forall v\in V: 1\cdot v=v$ --
			существование нейтрального по умножению элемента.
	\end{enumerate}
\end{define}

При поиске решения СЛАУ полагаются $K=\mathbb R$ и $V=\mathbb R^n$.

\begin{define}\label{eq:dot_production}
	\textbf{Скалярным произведением} векторов из линейного пространства
	$\mathbb R^n$ называется отображение $\cdot: \mathbb R^n\times
	\mathbb R^n\rightarrow \mathbb R$, удовлетворяющее следующим условиям:
	\begin{enumerate}[nosep]
		\item $\forall v\in R^n: (v,v)\ge 0$, причём $(v,v)=0
			\Leftrightarrow v=\overline{0}$ -- положительная
			определённость;
		\item $\forall u,v\in \mathbb R^n: (u,v)=(v,u)$ --
			коммутативность;
		\item $\forall \lambda,\mu \in\mathbb R^n,\;
			\forall u,v,w\in \mathbb R^n: (\lambda u+\mu v, w)=
			\lambda(u,w)+\mu(v,w)$ -- линейность.
	\end{enumerate}
\end{define}
\newpage

Тут специально было приведено определение для вещественных векторов, потому что
над полем комплекных чисел коммутативность заменяется на сопряжённость. При
работе со СЛАУ векторы вещественные.

\begin{define}\label{eq:vector_norm}
	\textbf{Нормой вектора} из линейного пространства $V$ над полем $K$
	называется отображение $\|\|: V\rightarrow K$, удовлетворяющее
	следующим условиям:
	\begin{enumerate}[nosep]
		\item $\forall v\in V: \|v\|\ge 0$, причём $\|v\|=0
			\Leftrightarrow v=\overline{0}$;
		\item $\forall \lambda\in K,\;\forall v\in V:
			\|\lambda v\|=|\lambda|\cdot\|v\|$;
		\item $\forall u,v\in V: \|u+v\|\le\|u\|+\|v\|$ -- неравенство
			треугольника.
	\end{enumerate}
\end{define}

\begin{define}\label{eq:some_vector_norms}
	Определим следующие нормы над линейным пространством $\mathbb R^n$:
	\begin{enumerate}[nosep]
		\item $\mathlarger{\|x\|_2=\sqrt{(x,x)_2}=\sqrt{\sum_{i=1}^{n}
			x_i^2}}$ -- \textbf{Евклидова норма};
		\item $\mathlarger{\|x\|_1=\sum_{i=1}^{n}|x_i|}$ --
			\textbf{октаэдрическая норма}; \\
		\item $\mathlarger{\|x\|_\infty=\max_{i\in\overline{1,n}}
			|x_i|}$ -- \textbf{кубическая норма}.
	\end{enumerate}
\end{define}

Несложно проверить, что это в действительности нормы.

\subsubsection{Ортонормированные базисы}
\begin{define}
	\textbf{Базис} -- максимальный по включению упорядоченный набор линейно
	независимых векторов линейного пространства $V$.
\end{define}

Напомним важное свойство базиса пространства.
\begin{theorem}
	Всякий вектор $v$ из линейного пространства $V$ представим в виде
	линейной комбинации базисных векторов данного пространства, и притом
	единственным образом.
\end{theorem}

\begin{define}
	\textbf{Ортонормированный базис} линейного пространства $V$ над полем
	$K$ -- базис $e_1,...,e_n$ такой, что
	\begin{enumerate}[nosep]
		\item $\forall i,j\in\overline{1,k}: (e_i,e_j)=\delta_{ij}$.
	\end{enumerate}
\end{define}

\subsection{Матрицы}
\begin{define}
	Матрица $A$ \textbf{положительно определённая} над линейным
	пространством $\mathbb R^n$, если для всякого ненулевого вектора
	$v\in \mathbb R^n$ верно $(Av, v)_2>0$.
\end{define}

\begin{define}
	\textbf{Норма матрицы} $\mathbb R^{n\times m}$
	называется отображение $\|\|: \mathbb R^{n\times m}\rightarrow
	\mathbb R$, удовлетворяющее следующим условиям:
	\begin{enumerate}[nosep]
		\item $\forall A\in\mathbb R^{n\times m}: \|A\|\ge 0$, причём
			$\|A\|=0\Leftrightarrow A=O$;
		\item $\forall \lambda\in \mathbb R,\;\forall v\in
			\mathbb R^{n\times m}:
			\|\lambda A\|=|\lambda|\cdot\|A\|$;
		\item $\forall A,B\in\mathbb R^{n\times m}: \|A+B\|\le\|A\|+\|B\|$ --
			неравенство треугольника.
	\end{enumerate}
\end{define}

\begin{define}
	Норма $\|A\|$ матрицы $A\in \mathbb R^{n\times n}$ называется
	\textbf{нормой, подчинённой векторной норме} $\|x\|$
	\eqref{eq:vector_norm}, где $x\in \mathbb R^n$, если верно
	\[\|A\|=\max_{x\ne \overline{0}}\frac{\|Ax\|}{\|x\|}.\]
\end{define}

\begin{lemma}
	Для всякой матричной нормы $\|A\|$, где $A\in\mathbb R^{n\times n}$,
	которая подчинена векторной норме $\|x\|$, где $x\in\mathbb R^n$, верны
	следующие неравенства:
	\begin{enumerate}[nosep]
		\item $\forall A\in\mathbb R^{n\times n},\;\forall x\in\mathbb
			R^n: \|Ax\|\le\|A\|\|x\|$;
		\item $\forall A,B\in\mathbb R^{n\times n}:
			\|AB\|\le\|A\|\|B\|$.
	\end{enumerate}
\end{lemma}

\begin{proof}
	Первое неравенство сразу доказывается из определения подчинённой нормы.

	Второе доказывается чуть дольше:
	\[\|AB\|=\max_{x\ne\overline{0}}\frac{\|A(Bx)\|}{\|x\|}\le
	\max_{x\ne\overline{0}}\frac{\|A\|\|Bx\|}{\|x\|}=\|A\|\|B\|.\]
\end{proof}

\begin{lemma}
	Явные формы матричных норм, подчинённых октаэдрической и кубическим
	нормам \eqref{eq:some_vector_norms}, имеют вид
	\[\|A\|_1=\max_{j\in\overline{1,n}}\sum_{i=1}^{n}|a_{ij}|,\qquad
	\|A\|_\infty=\max_{i\in\overline{1,n}}\sum_{j=1}^{n}|a_{ij}|.\]
\end{lemma}

\proofexercise

В этом преимущество этих норм: подчинённые им матричные нормы вычисляются
непосредственно по элементам матрицы $A$, чего не скажешь о Евклидовой норме.

\end{document}
