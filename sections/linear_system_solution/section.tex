\documentclass{article}

\begin{document}
\section{Вычислительные методы линейной алгебры}
Предмет исследования данного раздела -- это система линейных алгебраических
уравнений (СЛАУ)
\[
	\begin{cases}
		a_{11}x_1+a_{12}x_2+...+a_{1n}x_n=f_1, \\
		a_{21}x_1+a_{22}x_2+...+a_{2n}x_n=f_2, \\
		... \\
		a_{n1}x_1+a_{n2}x_2+...+a_{nn}x_n=f_n. \\
	\end{cases}
\]

Также можно записать систему в матричной форме как
\[Ax=f.\]

Если определитель матрицы $A$ отличен от нуля, то решение существует, и притом
только одно. Казалось, почему нельзя просто применить для решения метод Крамера
или обращение матрицы? Как и всегда: трудоёмкость этих методов быстро растёт с
ростом количества уравнений системы.

Например, для решения СЛАУ методом Крамера необходимо посчитать $n+1$
определитель матрицы, а вычислительная сложность этой операции \underline{в
худшем случае} составляет $O(n!)$. В итоге, общая вычислительная мощность
составляет примерно $O((n+1)!)$! Для понимания масштабов трагедии: если система
состоит всего из 20 уравнений, на одну атомарную операцию уходит $10^{-12}$
секунд, и вычисления не распараллеливаются и не векторизуются, то для решения
потребуется 1.62 года непрерывных вычислений, а также бесперебойной подачи
электроэнергии.

А количество уравнений в системе можеть быть гораздо больше двадцати. Поэтому
большие системы решают иначе. В этом разделе будут рассмотрены метод Гаусса и
итерационные методы.

Кстати, в 2010 году удалось реализовать метод Крамера с вычислительной
сложностью $O(n^3)$, что сопоставимо со сложностью метода Гаусса.

\newpage

\subsection{Линейные пространства}
Прежде чем как начать разбирать методы решения систем линейных уравнений,
освежим знания из линейной алгебры.

\begin{define}
	Пространство $L$ над полем $K$, в котором определены операции сложения
	$+: V^2\rightarrow V$ и умножения $*: K\times V\rightarrow V$,
	называется \textbf{линейным пространством}, если в нём верны следующие
	аксиомы:

	\begin{enumerate}[nosep]
		\item $\forall u,v\in V\;\; u+v=u+v$ -- коммутативность
			сложения,
		\item $\forall u,v,w\in V\;\; (u+v)+w=u+(v+w)$ --
			ассоциативность сложения,
		\item $\exists\overline{0}: \forall v\in V\;\;
			v+\overline{0}=\overline{0}+v=v$ -- существование
			нейтрального по сложению элемента,
		\item $\forall v\in V\;\;\exists (-v): v+(-v)=(-v)+v=
			\overline{0}$ -- существование противоположного
			элемента,
		\item $\forall\lambda\in K,\;\forall u,v\in V\;\; \lambda(u+v)=
			\lambda u+\lambda v$ -- дистрибутивность умножения
			относительно сложения векторов,
		\item $\forall\lambda,\mu\in K,\;\forall v\in V\;\;
			(\lambda+\mu)v=\lambda v+\mu v$ -- дистрибутивность
			умножения,
		\item $\forall\lambda,\mu\in K,\;\;\forall v\in V\; (\lambda\mu)
			v=\lambda(\mu v)$ -- ассоциативность умножения,
		\item $\exists 1\in K: \forall v\in V\;\; 1\cdot v=v$ --
			существование нейтрального по умножению элемента.
	\end{enumerate}
\end{define}

При поиске решения СЛАУ полагаются $K=\mathbb R$ и $V=\mathbb R^n$.

\begin{define}\label{eq:dot_production}
	\textbf{Скалярным произведением} векторов из линейного пространства
	$\mathbb R^n$ называется отображение $\cdot: \mathbb R^n\times
	\mathbb R^n\rightarrow \mathbb R$, удовлетворяющее следующим условиям:
	\begin{enumerate}[nosep]
		\item $\forall v\in R^n\;\; (v,v)\ge 0$, причём $(v,v)=0
			\Leftrightarrow v=\overline{0}$ -- положительная
			определённость;
		\item $\forall u,v\in \mathbb R^n\;\; (u,v)=(v,u)$ --
			коммутативность;
		\item $\forall \lambda,\mu \in\mathbb R^n,\;
			\forall u,v,w\in \mathbb R^n\;\; (\lambda u+\mu v, w)=
			\lambda(u,w)+\mu(v,w)$ -- линейность.
	\end{enumerate}
\end{define}
\newpage

Тут специально было приведено определение для вещественных векторов, потому что
над полем комплекных чисел коммутативность заменяется на сопряжённость. При
работе со СЛАУ векторы вещественные.

\begin{define}\label{eq:vector_norm}
	\textbf{Нормой вектора} из линейного пространства $V$ над полем $K$
	называется отображение $\|\|: V\rightarrow K$, удовлетворяющее
	следующим условиям:
	\begin{enumerate}[nosep]
		\item $\forall v\in V\;\; \|v\|\ge 0$, причём $\|v\|=0
			\Leftrightarrow v=\overline{0}$;
		\item $\forall \lambda\in K,\;\forall v\in V\;\;
			\|\lambda v\|=|\lambda|\cdot\|v\|$;
		\item $\forall u,v\in V\;\; \|u+v\|\le\|u\|+\|v\|$ --
			неравенство треугольника.
	\end{enumerate}
\end{define}

\begin{define}\label{eq:some_vector_norms}
	Определим следующие нормы над линейным пространством $\mathbb R^n$:
	\begin{enumerate}[nosep]
		\item $\mathlarger{\|x\|_2=\sqrt{(x,x)_2}=\sqrt{\sum_{i=1}^{n}
			x_i^2}}$ -- \textbf{Евклидова норма};
		\item $\mathlarger{\|x\|_1=\sum_{i=1}^{n}|x_i|}$ --
			\textbf{октаэдрическая норма}; \\
		\item $\mathlarger{\|x\|_\infty=\max_{i\in\overline{1,n}}
			|x_i|}$ -- \textbf{кубическая норма}.
	\end{enumerate}
\end{define}

Несложно проверить, что это в действительности нормы.

\subsubsection{Ортонормированные базисы}
\begin{define}
	\textbf{Базис} -- максимальный по включению упорядоченный набор линейно
	независимых векторов линейного пространства $V$.
\end{define}

Напомним важное свойство базиса пространства.
\begin{theorem}
	Всякий вектор $v$ из линейного пространства $V$ представим в виде
	линейной комбинации базисных векторов данного пространства, и притом
	единственным образом.
\end{theorem}

\begin{define}
	\textbf{Ортонормированный базис} линейного пространства $V$ над полем
	$K$ -- базис $e_1,...,e_n$ такой, что
	$\forall i,j\in\overline{1,k}: (e_i,e_j)=\delta_{ij}$.
\end{define}

\subsubsection{Матрицы}
\begin{define}
	Матрица $A$ \textbf{положительно определённая} над линейным
	пространством $\mathbb R^n$, если для всякого ненулевого вектора
	$v\in \mathbb R^n$ верно $(Av, v)_2>0$.
\end{define}

\begin{define}
	\textbf{Нормой матрицы} $\mathbb R^{n\times m}$
	называется отображение $\|\|: \mathbb R^{n\times m}\rightarrow
	\mathbb R$, удовлетворяющее следующим условиям:
	\begin{enumerate}[nosep]
		\item $\forall A\in\mathbb R^{n\times m}\;\; \|A\|\ge 0$, причём
			$\|A\|=0\Leftrightarrow A=O$;
		\item $\forall \lambda\in \mathbb R,\;\forall v\in
			\mathbb R^{n\times m}\;\;
			\|\lambda A\|=|\lambda|\cdot\|A\|$;
		\item $\forall A,B\in\mathbb R^{n\times m}\;\; \|A+B\|\le
			\|A\|+\|B\|$ -- неравенство треугольника.
	\end{enumerate}
\end{define}

\begin{define}
	Норма $\|A\|$ матрицы $A\in \mathbb R^{n\times n}$ называется
	\textbf{нормой, подчинённой векторной норме} $\|x\|$
	\eqref{eq:vector_norm}, где $x\in \mathbb R^n$, если верно
	\[\|A\|=\max_{x\ne \overline{0}}\frac{\|Ax\|}{\|x\|}.\]
\end{define}

\begin{lemma}\label{eq:subordinate_norm_properties}
	Для всякой матричной нормы $\|A\|$, где $A\in\mathbb R^{n\times n}$,
	которая подчинена векторной норме $\|x\|$, где $x\in\mathbb R^n$, верны
	следующие неравенства:
	\begin{enumerate}[nosep]
		\item $\forall A\in\mathbb R^{n\times n},\;\forall x\in\mathbb
			R^n\;\; \|Ax\|\le\|A\|\|x\|$;
		\item $\forall A,B\in\mathbb R^{n\times n}\;\;
			\|AB\|\le\|A\|\|B\|$.
	\end{enumerate}
\end{lemma}

\begin{proof}
	Первое неравенство. Если $x=\overline{0}$, то неравенство верно. Иначе,
	по определению подчинённой нормы,
	\[\|A\|=\max_{y\ne\overline{0}}\frac{\|Ay\|}{\|y\|}\ge
	\frac{\|Ax\|}{\|x\|}\;\Leftrightarrow\;
	\|Ax\|\le\|A\|\|x\|.\]

	Второе доказывается чуть дольше:
	\[\|AB\|=\max_{x\ne\overline{0}}\frac{\|A(Bx)\|}{\|x\|}\le
	\max_{x\ne\overline{0}}\frac{\|A\|\|Bx\|}{\|x\|}=\|A\|\|B\|.\]
\end{proof}

\begin{lemma}
	Явные формы матричных норм, подчинённых октаэдрической и кубическим
	нормам \eqref{eq:some_vector_norms}, имеют вид
	\[\|A\|_1=\max_{j\in\overline{1,n}}\sum_{i=1}^{n}|a_{ij}|,\qquad
	\|A\|_\infty=\max_{i\in\overline{1,n}}\sum_{j=1}^{n}|a_{ij}|.\]
\end{lemma}

\proofexercise

В этом преимущество этих норм: подчинённые им матричные нормы вычисляются
непосредственно по элементам матрицы $A$, чего не скажешь о Евклидовой норме.

\begin{define}
	Скаляр $\lambda$ и ненулевой вектор $v$ называются \textbf{собственным
	значением} и \textbf{собственным вектором} матрицы $A$ соответственно,
	если для них выполняется равенство
	\[Av=\lambda v.\]
\end{define}

У собственных значений симметричных матриц есть одно замечательное свойство.
\begin{lemma}\label{eq:symmetric_matrix_props}
	Если матрица $A$ симметричная, то все её собственные значения
	действительные, а из собственных векторов можно построить
	ортонормированный базис.
\end{lemma}

\begin{theorem}[о Евклидовой норме симметричной матрицы]
\label{eq:symmetric_matrix_norm}
	Если матрица $A$ симметричная, то её Евклидова норма равна $\|A\|_2=
	\max|\lambda(A)|$, где $\lambda(A)$ -- спектр матрицы, или же множество
	её собственных значений.
\end{theorem}

\begin{corollary}
	Если матрица $A$ симметричная и не вырождена, то Евклидова норма
	обратной ей матрицы равна
	$\mathlarger{\|A^{-1}\|_2=\frac{1}{\min|\lambda(A)|}}$.
\end{corollary}

\proofexercise

\subsection{Система линейных алгебраических уравнений}
К решению систем линейных алгебраических уравнений сводятся практически все
задачи вычислительной математики.

Прежде чем переходить к изложению собственно методов, проведём краткое
исследование основных свойств самой системы линейных алгебраических уравнений. В
частности, чувствительности её решения к малым изменениям входных данных.

\begin{define}
	Будем считать, что задача поставлена \textbf{корректно}, если:
	\begin{enumerate}[nosep]
		\item Решение $x$ существует;
		\item Решение $x$ единственно;
		\item Решение $x$ непрерывно зависит от входных данных.
	\end{enumerate}
\end{define}

Если первые 2 пункта выполняются при $det(A)\ne 0$, так как тогда существует
обратная матрица, то третий пункт нужно рассмотреть поподробней.

\begin{theorem}[об оценке погрешности решения СЛАУ]
	Если $\delta f$ -- погрешность $f$ из системы линейных алгебраических
	уравнений $Ax=f$, матрица $A$ невырожденная, то погрешность решения
	$\delta x$ имеет оценку
	\[\boxed{\frac{\|\delta x\|}{\|x\|}\le\nu(A)\frac{\|\delta f\|}
	{\|f\|}},\]
	где $\nu(A)=\|A\|\|A^{-1}\|$ -- \textbf{число обусловленности}.
\end{theorem}

\begin{proof}
	Рассмотрим следующее равенство:
	\[A\delta x=\delta f.\]
	Поскольку матрица невырожденная, её можно обратить и перенести во вторую
	половину равенства:
	\[\delta x=A^{-1}\delta f.\]

	Равенство значений означает равенство норм. Воспользуемся оценкой по
	лемме \eqref{eq:subordinate_norm_properties} и получим
	\[\|\delta x\|\le\|A^{-1}\|\|\delta f\|.\]

	К исходному равенству $Ax=f$ применяем ту же оценку:
	\[\|f\|\le\|A\|\|x\|.\]

	Перемножим эти неравенства:
	\[\|\delta x\|\|f\|\le\|A\|\|A^{-1}\|\|x\|\|\delta f\|.\]

	Из него и следует оценка.
\end{proof}

Если $\nu(A)$ велико, то лишь очень малые погрешности входных данных гарантируют
малую погрешность решения. В связи с этим, про матрицы, у которых $\nu(A)$
велико, говорят, что они плохо обусловлены, а матрицы с относительно малой
величиной $\nu(A)$ называют хорошо обусловленными.

\begin{example}
	Рассмотрим матрицу
	$A=
		\begin{pmatrix}
			1	& 1 \\
			1	& 1+10^{-4} \\
		\end{pmatrix}
	$
	и систему линейных уравнений с ней
	\[
		\begin{pmatrix}
			1	& 1 \\
			1	& 1+10^{-4} \\
		\end{pmatrix}
		\begin{pmatrix}
			2 \\
			0 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			2 \\
			2 \\
		\end{pmatrix}
		.
	\]

	Теперь мы испортим систему, немного изменив $f$:
	\[f+\delta f=
		\begin{pmatrix}
			2 \\
			2+10^{-4} \\
		\end{pmatrix}
		\quad\Leftrightarrow\quad
		x+\delta x=
		\begin{pmatrix}
			1 \\
			1 \\
		\end{pmatrix}
	\]

	Совсем небольшое изменение условия привело к огромному изменению
	решения системы. Такую систему решать нельзя.

	Теперь найдём обусловленность матрицы $A$ по Евклидовой норме.
	Воспользуемся тем, что матрица симметричная
	\eqref{eq:symmetric_matrix_norm}, тогда $\|A\|_2\approx 2$.
	Норма же обратной матрицы, которая тоже симметричная,
	$\|A^{-1}\|_2\approx 2\cdot 10^4$. И тогда $\nu(A)\approx
	4\cdot 10^4$, то есть матрица плохо обусловленная. Результат был
	ожидаем.
\end{example}

\subsection{Методы Гаусса}
Метод Крамера далеко не единственный прямой метод решения системы линейных
алгебраических уравнений.

Метод Гаусса основан на том, чтобы элементарными преобразованиями привести
систему виду, который проще решать. Сюда относятся такие ''школьные'' методы
решения СЛАУ, как сложение уравнений и умножение их на ненулевой скаляр. Методы
Гаусса имеют вычислительную сложность $O(n^3)$ -- при такой сложности система
с 20 уравнениями решится за 8 нс, если применить условия из начала этого
раздела.

Сначала дадим некоторые виды ''хороших'' матриц.

\begin{define}
	Матрица $A$ называется \textbf{диагональной} и обозначается как $D$,
	если все её ненулевые элементы расположены на главной диагонали.
\end{define}

Если в системе уравнений $Ax=f$ матрица $A$ диагональная, то есть
\[
	\begin{pmatrix}
		a_{11}	& 0	& ...	& 0 \\
		0	& a_{22}& ...	& 0 \\
		\vdots	& \vdots&\ddots &\vdots \\
		0	& 0	& ...	& a_{nn} \\
	\end{pmatrix}
	\cdot
	\begin{pmatrix}
		x_1 \\
		x_2 \\
		... \\
		x_n \\
	\end{pmatrix}
	=
	\begin{pmatrix}
		f_1 \\
		f_2 \\
		... \\
		f_n \\
	\end{pmatrix}
	,
\]

то очевидно, что $x_i=\frac{f_i}{a_{11}}$.

\begin{define}
	Матрица $A$ называется \textbf{нижнетреугольной} и обозначается как $L$,
	если у неё нет ненулевых элементов \underline{выше} главной диагонали.
\end{define}

\newpage

Если в системе уравнений $Ax=f$ матрица $A$ нижетреугольная, то есть
\[
	\begin{pmatrix}
		a_{11}	& 0	& ...	& 0 \\
		a_{21}	& a_{22}& ...	& 0 \\
		\vdots	& \vdots&\ddots &\vdots \\
		a_{n1}	& a_{n2}& ...	& a_{nn} \\
	\end{pmatrix}
	\cdot
	\begin{pmatrix}
		x_1 \\
		x_2 \\
		... \\
		x_n \\
	\end{pmatrix}
	=
	\begin{pmatrix}
		f_1 \\
		f_2 \\
		... \\
		f_n \\
	\end{pmatrix}
	,
\]

то решение системы вычисляется по рекуррентной формуле следующим образом:
\[x_1=\frac{f_1}{a_{11}},\]
\[x_k=\frac{1}{a_{kk}}\Big(f_i-\mathlarger{\sum_{i=1}^{k-1}a_{ki}x_i}\Big),
\quad k\in\overline{2,n}.\]

\begin{define}
	Матрица $A$ называется \textbf{верхнетреугольной} и обозначается как
	$U$, если у неё нет ненулевых элементов \underline{ниже} главной
	диагонали.
\end{define}

Решение системы $Ax=f$ с верхнетреугольной матрицей строится аналогично решению
системы с нижнетреугольной, только значения вычисляются снизу вверх.

\begin{theorem}[об LU-разложении]
	Всякую невырожденную квадратную матрицу $A$ можно представить в виде
	произведения нижне- и верхнетреугольных матриц, то есть
	\[\forall A: det(A)\ne 0\quad\exists L,U: A=LU.\]
\end{theorem}

\subsubsection{Схема единственного деления}
\begin{define}
	\textbf{Угловой минор} матрицы $A$ порядка $k$ -- минор, который
	получается выделением первых $k$ строк и столбцов. Обозначается
	как $\Delta_k$.
\end{define}

\begin{algorithm}[метод Гаусса со схемой единственного деления]
\label{eq:gauss_single_division}
	Пусть необходимо решить уравение $Ax=f$. Допустим, что все угловые
	миноры матрицы $A:\Delta_i\ne 0$.

	На первом шаге первая строка матрицы делится на $\Delta_1=a_{11}$.
	Затем полученная строка вычитается из всех остальных так, чтобы занулить
	весь первый столбец ниже главной диагонали.

	На $k$-ом шаге $k$-я строка матрицы делится на
	$\frac{\Delta_k}{\Delta_{k-1}}$, а затем она вычитается из всех
	следующих уравнений так, чтобы занулить $k$ столбец ниже главной
	диагонали.

	После $n$ шагов мы получим верхнетреугольную матрицу, система с которой
	легко решается.
\end{algorithm}

\begin{example}
	Решим систему уравнений
	\[
		\begin{pmatrix}
			-6	& 1	& 1	\\
			3	& -1	& -2	\\
			1	& 3	& 3	\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
			x_3 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			-9 \\
			-1 \\
			11 \\
		\end{pmatrix}
	\]
	методом Гаусса со схемой единственного деления.

	Угловые миноры равны:
	\[\Delta_1=-6,\qquad\qquad\;\]
	\[\Delta_2=
		\begin{vmatrix}
			-6	& 1	\\
			3	& -1	\\
		\end{vmatrix}
		=3,
	\]
	\[\Delta_3=det(A)=-19.\]

	Разделим первое уравнение на $\Delta_1$ и вычтем его из второго и
	третьего так, чтобы занулить первый столбец ниже диагонали:
	\[
		\begin{pmatrix}
			1	&-\frac{1}{6}	& -\frac{1}{6}	\\
			3	& -1	& -2	\\
			1	& 3	& 3	\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
			x_3 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			\frac{3}{2} \\
			-1 \\
			11 \\
		\end{pmatrix}
		\quad\Leftrightarrow\quad
		\begin{pmatrix}
			1	&-\frac{1}{6}	& -\frac{1}{6}	\\
			0	& -\frac{1}{2}	& -\frac{3}{2}	\\
			0	& \frac{19}{6}	& \frac{19}{6}	\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
			x_3 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			\frac{3}{2} \\
			-\frac{11}{2} \\
			\frac{19}{2} \\
		\end{pmatrix}
	\]

	В получившейся системе, разделим второе уравнение на
	$\frac{\Delta_2}{\Delta_1}=-\frac{1}{2}$ и вычтем его так, чтобы
	занулить второй столбец ниже диагонали:
	\[
		\begin{pmatrix}
			1	&-\frac{1}{6}	& -\frac{1}{6}	\\
			0	& 1	& 3	\\
			0	& \frac{19}{6}	& \frac{19}{6}	\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
			x_3 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			\frac{3}{2} \\
			11 \\
			\frac{19}{2} \\
		\end{pmatrix}
		\quad\Leftrightarrow\quad
		\begin{pmatrix}
			1	&-\frac{1}{6}	& -\frac{1}{6}	\\
			0	& 1	& 3	\\
			0	& 0	& -\frac{19}{3}	\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
			x_3 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			\frac{3}{2} \\
			11 \\
			-\frac{76}{3} \\
		\end{pmatrix}
	\]

	Наконец, делим третье уравнение на $\frac{\Delta_2}{\Delta_1}=
	-\frac{19}{3}$:
	\[
		\begin{pmatrix}
			1	&-\frac{1}{6}	& -\frac{1}{6}	\\
			0	& 1	& 3	\\
			0	& 0	& 1	\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
			x_3 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			\frac{3}{2} \\
			11 \\
			4 \\
		\end{pmatrix}
	\]

	Получили верхнетреугольную матрицу. Вот окончательное решение системы:
	\[
		\begin{pmatrix}
			x_1 \\
			x_2 \\
			x_3 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			2 \\
			-1 \\
			4 \\
		\end{pmatrix}
	\]
\end{example}

\subsubsection{Метод в выбором главного элемента по столбцу}
Данный метод является улучшением предыдущего.

\begin{algorithm}[Метод Гаусса с выбором главного элемента по столбцу]
	Пусть необходимо решить уравнение $Ax=f$. Прежде чем как начать,
	поместим первой строкой то уравнение, которе имеет наибольший по модую
	коэффициент перед $x_1$.

	Дальнейшие шаги аналогичны предыдущему методу
	\eqref{eq:gauss_single_division}.
\end{algorithm}

Давайте посмотрим, что может произойти, если не сделать этот дополнительный шаг.

\begin{example}
	Решим систему уравнений
	\[
		\begin{pmatrix}
			10^{-8}	& 1	\\
			1	& 1	\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			1 \\
			2 \\
		\end{pmatrix}
	\]

	Умножим первое уравнение на $10^8$ и вычтем его из второго:
	\[
		\begin{pmatrix}
			1	& 10^8	\\
			1	& 1	\\
		\end{pmatrix}
		\begin{pmatrix}	
			x_1 \\
			x_2 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			10^8 \\
			2    \\
		\end{pmatrix}
		\quad\Leftrightarrow\quad
		\begin{pmatrix}
			1	& 10^8	\\
			0	&1-10^8	\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			10^8 \\
			2-10^8 \\
		\end{pmatrix}
	\]

	Разделим на $2-10^8$ второе уравнение и получим верхнетреугольную
	матрицу:
	\[
		\begin{pmatrix}
			1	& 10^8	\\
			0	& 1	\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			10^8 \\
			\frac{2-10^8}{1-10^8} \\
		\end{pmatrix}
	\]

	Округлив деление, мы получим, что
	\[
		\begin{pmatrix}
			x_1 \\
			x_2 \\
		\end{pmatrix}
		\approx
		\begin{pmatrix}
			0 \\
			1 \\
		\end{pmatrix}
	\]

	Но это даже близко не решение системы. Что пошло не так? Обусловленность
	матрицы $\nu_1(A)\approx 4$, что вполне неплохое значение. На деле, мы
	испортили обусловленность диагональной матрицы: $\nu_1(D_1)\approx
	10^{16}$, что является чудовищно огромным значением!

	Кстати, для справки: здесь погрешность вычислений меньше погрешности
	вещественного типа $float$.

	Попробум решить эту систему, но поменяем уравнения местами
	\[
		\begin{pmatrix}
			1	& 1	\\
			10^{-8}	& 1	\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			2 \\
			1 \\
		\end{pmatrix}
	\]

	Делить первое уравнение ни на что не надо, так что сразу вычтем его
	из второго:
	\[
		\begin{pmatrix}
			1	& 1	\\
			0	&1-10^{-8}\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			2 \\
			1-2\cdot 10^{-8} \\
		\end{pmatrix}
	\]

	И приведём систему к верхнетреугольному виду:
	\[
		\begin{pmatrix}
			1	& 1	\\
			0	& 1	\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			2 \\
			\frac{1-2\cdot 10^{-8}}{1-10^{-8}} \\
		\end{pmatrix}
	\]

	Округлив деление, мы получим, что
	\[
		\begin{pmatrix}
			x_1 \\
			x_2 \\
		\end{pmatrix}
		\approx
		\begin{pmatrix}
			1 \\
			1 \\
		\end{pmatrix}
	\]

	Это очень близко к точному значению. Что за магия обеспечила хорошую
	аппроксимацию решения? Всё просто: $\nu_1(D_2)=4$, то есть
	обусловленность получившейся верхнетреугольной матрицы практически не
	изменилась.
\end{example}
\leavevmode

\subsection{Метод прогонки}
Трёхдиагональные матрицы, которые зачастую возникают из неявных разностных схем,
можно решить ещё проще; если точней, то за $O(n)$.

\begin{algorithm}[метод прогонки]
	Пусть необходимо решить уравнение $Ax=f$, где $A$ -- трёхдиагональная
	матрица. Обозначим элементы матрицы
	под главной диагональю $a_{i,i-1}=A_i$, при этом полагая $A_0=0$,
	элементы на диагонали $a_{i,i}=B_i$,
	и элементы над диагональю $a_{i,i+1}=C_i$, при этом полагая $C_n=0$,
	тогда матрица примет вид
	\[
		A=
		\begin{pmatrix}
			B_1	& C_1	& 0	& 0	& ...	& 0 \\
			A_2	& B_2	& C_2	& 0	& ...	& 0 \\
			0	& A_3	& B_3	& C_3	& ...	& \vdots\\
			\vdots	& \vdots& \vdots& \ddots& \ddots& C_{n-1} \\
			0	& 0	& 0	& ...	& A_n	& B_n\\
		\end{pmatrix}
	\]

	\textbf{Прямой ход}.
	Введём наборы коэффициентов $\alpha_i$ и $\beta_i$. На первом шагу
	\[\alpha_1=\frac{-C_1}{B_1},\quad \beta_1=\frac{f_1}{B_1},\]
	А на следующих шагах зададим их рекуррентно:
	\[\alpha_i=\frac{-C_i}{B_i+\alpha_{i-1}A_i},\quad
	\beta_i=\frac{f_i-\beta_{i-1}A_i}{B_i+\alpha_{i-1}A_i}.\]

	\textbf{Обратный ход}.
	Перейдём от этих коэффициентов непосредственно к решению системы:
	\[x_n=\beta_n,\]
	\[x_i=\alpha_i x_{i+1}+\beta_i,\quad i\in\overline{1,n-1}.\]

	Решение системы найдено.
\end{algorithm}

Теперь исследуем метод на устойчивость.

\begin{define}\label{eq:diagonally_dominant_matrix}
	Говорят, что квадратная матрица $A$ обладает \textbf{диагональным
	преобладанием}, если
	\[\forall i\in\overline{1,n}\quad |a_{ii}|>\sum_{j\ne i}|a_{ij}|.\]
\end{define}

\begin{theorem}
	Если для трёхдиагональной матрицы $A$ выполнено условие диагонального
	преобладания, причём
	\[0<\varepsilon<\min_{i\in\overline{1,n}}\Big(|a_{ii}|-
	\sum_{j\ne i}|a_{ij}|\Big),\]
	тогда для любого решения СЛАУ $Ax=f$ справедливо
	\[\boxed{\|x\|_\infty\le\frac{1}{\varepsilon}
	\|f\|_\infty}.\]
\end{theorem}

\begin{example}
	Решим систему линейных алгебраических уравнений
	\[
		\begin{pmatrix}
			-3	& 2	& 0	& 0	\\
			4	& 9	& 4	& 0	\\
			0	& -1	& 4	& 2	\\
			0	& 0	& -2	& 7	\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
			x_3 \\
			x_4 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			21  \\
			-1  \\
			-13 \\
			-3  \\
		\end{pmatrix}
	\]

	Для этой матрицы выполнено условие диагонального преобладания, что
	гарантирует успешность использования метода прогонки.

	Считать мы будем немного иначе: мы будем от всех значений оставлять
	только до 3-х значащих цифр, а остальное округлять, чтобы сделать
	имитацию погрешности округлений в памяти компьютера.

	Прямой ход:

	\begin{tabular}{*{2}{l}}
		\vspace{4mm}
		$\mathlarger{\alpha_1=-\frac{2}{-3}=0.667}$, &
			$\mathlarger{\beta_1=\frac{21}{-3}=-7}$, \\
		\vspace{4mm}
		$\mathlarger{\alpha_2=\frac{-4}{9+0.667\cdot 4}}=-0.342$, &
			$\mathlarger{\beta_2=\frac{-1-(-7)\cdot 4}{9+0.667\cdot 4}=
			2.31}$, \\
		\vspace{4mm}
		$\mathlarger{\alpha_3=\frac{-2}{4+(-0.342)\cdot (-1)}}=-0.461$, &
			$\mathlarger{\beta_3=\frac{-13-(2.31)\cdot (-1)}
			{4+(-0.342)\cdot (-1)}=-2.47}$, \\
		$\mathlarger{\alpha_4=0}$, &
			$\mathlarger{\beta_4=x_4=\frac{-3-(-2.47)\cdot (-2)}
			{7+(-0.461)\cdot (-2)}=-1.00}$. \\
	\end{tabular}

	А теперь и обратный ход:
	\[x_3=(-0.461)\cdot (-1.00)+(-2.47)=-2.01,\]
	\[x_2=(-0.342)\cdot (-2.01)+2.31=3.00,\]
	\[x_1=0.667\cdot 3.00+(-7)=-5.00.\]

	Несмотря на погрешность, вычисленное решение совпало с точным, не считая
	совсем небольшой погрешности $x_3$. На практике, в ходе прогонки больших
	систем может накопиться довольно существенная погрешность.
\end{example}

\subsection{Итерационные методы}
Прямые методы всё равно так или иначе имеют очень большую асимптотику, чтобы
использовать их для решения больших систем.

В итерационных методах решение $x$ системы линейных алгебраических уравнений
$Ax=f$ находится как предел при $m\to\infty$ последовательных приближений $x^m$,
где $m$ -- номер итерации (приближения).

Поступим так же, как при решении нелинейных уравнений мы сводили задачу $f(x)=0$
к задаче поиска неподвижной точки $\varphi(x)=x$. Обозначим $F(x)=Ax-f$, тогда
\[F(x)=0\quad\Leftrightarrow\quad -\tau B^{-1}F(x)=0\quad\Leftrightarrow\quad
\underset{\Phi(x)}{\underbrace{x-\tau B^{-1}F(x)}}=x,\]
где $\tau$ -- некоторое ненулевое число, а $B$ -- невырожденная матрица.

\begin{define}\label{eq:sle_iterative_method}
	Пусть $x^0$ -- некоторое начальное значение, $\Phi(x)=x-\tau
	B^{-1}(Ax-f)$, где $\tau$ -- некоторое ненулевое число, а $B$ --
	невырожденная матрица, тогда последовательность $x^m$, заданная
	рекуррентной формулой
	\[x^{m+1}=\Phi(x^m),\]
	а также сама эта формула, называются \textbf{итерационной
	последовательностью} и \textbf{итерационным процессом (методом)}
	соответственно.
\end{define}

\begin{define}\label{eq:static_canon_process}
	Виды итерационного процесса
	\[x^{m+1}=x^m-\tau B^{-1}(Ax^m-f)\quad\Leftrightarrow\quad
	B\frac{x^{m+1}-x^m}{\tau}+Ax^m=f\]
	называются каноническими видами \textbf{стационарного} двуслойного
	итерационного процесса.
\end{define}

\begin{define}
	Итерационный процесс
	\[B\frac{x^{m+1}-x^m}{\tau_{m+1}}+Ax^m=f,\]
	где параметр $\tau$ зависит от номера итерации, называется каноническими
	видами \textbf{нестационарного} двуслойного итерационного процесса.
\end{define}

\begin{define}
	Пусть $x^m$ -- некоторая итерационная последовательность, составленная
	в ходе решения системы $Ax=f$.

	Вектор $z^m=x^m-x$ называется \textbf{вектором погрешности}, где $x$ --
	точное решение системы.

	Вектор $r^m=Ax^m-f$ называется \textbf{вектором невязки}.
\end{define}

Очевидно, что $r^m=Az^m$.

\newpage

\begin{define}
	Итерационный метод и образонанная им итерационная последовательность
	$x^m$ называются сходящимися, если верно
	\[\lim_{n\to\infty}x^n=x\text{ или, если иначе, }
	\lim_{n\to\infty}z^n=\overline{0}.\]
\end{define}

Параметры $B$ и $\tau$ выбирают так, чтобы, с одной стороны, обеспечить
сходимость итерационного процесса, а с другой, -- чтобы итерационный процесс
считался проще, чем прямые методы решения системы. Как и при решении нелинейных
уравнений, здесь применяется принцип сжимающих отображений
\eqref{eq:banach_fp_theorem} (да-да, это ссылка на 100 с лишним страниц
назад!!!).

\begin{define}
	Матрица $S=E-\tau B^{-1}A$ называется \textbf{матрицей перехода}.
\end{define}

С таким обозначением мы можем записать итерационный процесс
\eqref{eq:sle_iterative_method} как $x^{m+1}=Sx^m+\tau B^{-1}f$.

\begin{theorem}[достаточное условие сходимости итерационного процесса]
\label{eq:sle_matrix_norm_cond}
	Чтобы стационарный итерационный процесс
	\eqref{eq:static_canon_process}, приближающий решение СЛАУ $Ax=f$,
	сходился, достаточно, чтобы в какой-нибудь подчинённой норме было верно
	$\|S\|<1$, где $S=E-\tau B^{-1}A$ -- матрица перехода.
\end{theorem}

\begin{proof}
	Отображение $\Phi(x)=Sx+\tau B^{-1}f$ переводит полное метрическое
	пространство $\mathbb R^n$ в себя. При этом метрика в $\mathbb R^n$
	задаётся векторной нормой, которой подчинена норма матрицы.

	Далее, для любых векторов $x$ и $y$ справедливо
	\[\|\Phi(x)-\Phi(y)\|=\|Sx+\tau B^{-1}f-(Sy+\tau B^{-1}f)\|=
	\|S(x-y)\|.\]

	Пусть $\|S\|=q<1$, тогда
	\[\|\Phi(x)-\Phi(y)\|=\|S(x-y)\|\le\|S\|\|x-y\|=q\|x-y\|.\]

	Тогда по определению $\Phi(x)$ является сжимающим отображением, что
	означает, что соответствующий ему итерационный процесс сходится.
\end{proof}

\begin{lemma}
	В итерационном процессе справедливо равенство
	\[z^{m+1}=Sz^m,\]
	где $S$ -- матрица перехода.
\end{lemma}

\begin{proof}
	Запишем каноническую форму стационарного итерационного процесса:
	\[x^{m+1}=x^m-\tau B^{-1}(Ax^m-f).\]
	Вычтем из обеих частей равенство точное решение $x$, а $f$ заменим на
	$Ax$:
	\[z^{m+1}=z^m-\tau B^{-1}(Ax^m-Ax)=z^m-\tau B^{-1}A(x^m-x)=\]
	\[=z^m-\tau B^{-1}Az^m=Sz^m.\]
\end{proof}

\begin{theorem}[критерий сходимости итерационного процесса]
\label{eq:sle_convergence_criterion}
	Итерационный метод сходится тогда и только тогда, когда все собственные
	значения матрицы перехода $S=(E-\tau B^{-1}A)$ по модулю меньше единицы:
	\[|\lambda (S)|<1.\]
\end{theorem}

Доказательство в общем случае довольно сложное, поэтому докажем критерий лишь
для симметричной матрицы перехода.

\begin{proof}
	Так как матрица перехода $S$ симметрична, то по лемме
	\eqref{eq:symmetric_matrix_props} все её собственные числа
	действительные и из её собственных векторов можно построить базис
	$v_1,...,v_n$. Через их линейную комбинацию запишем вектор $z^0$:
	\[z^0=\sum_{i=1}^{n}\alpha_iv_i.\]

	Запишем соотношение $z^{m+1}=Sz$ при $m=0$, подставив в него полученное
	представление $z^0$ и учтём, что $Sv_i=\lambda_iv_i$:
	\[z^1=Sz^0=S\Big(\sum_{i=1}^{n}\alpha_iv_i\Big)=
	\sum_{i=1}^{n}\alpha_iSv_i=
	\sum_{i=1}^{n}\alpha_i\lambda_iv_i.\]

	По индукции мы имеем
	\[z^m=\sum_{i=1}^{n}\alpha_i\lambda_i^mv_i.\]

	Теперь условие сходимости итерационного процесса становится очевидным:
	\[\lim_{m\to\infty}z^m=\overline{0}\quad\Leftrightarrow\quad
	\forall i\in\overline{1,n}\;\;\lim_{m\to\infty}\lambda_i^m=0\quad
	\Leftrightarrow\quad \forall i\in\overline{1,n}\;\;|\lambda_i|<1.\]
\end{proof}

Есть ещё одно условие, при котором итерационный процесс сходится.

\begin{theorem}[Самарского]
	Если матрица $A$ симметричная и положительная определённая и $\tau>0$,
	то при условии
	\[B>\frac{1}{2}\tau A\]
	стационарный итерационный процесс \eqref{eq:static_canon_process}
	сходится.
\end{theorem}

Докажем эту теорему для симметричной матрицы $B$.

\begin{proof}
	Запись $B>\frac{1}{2}\tau A$ означает, что матрица $B-\frac{1}{2}\tau A$
	положительно определённая, то есть
	\[\forall x\ne\overline{0}\;\; \big((B-0.5\tau A)x,x\big)>0,\]
	\[(Bx,x)>0.5(Ax,x).\]

	Для любого собственного вектора $v_i$ и отвечающего ему собственного
	значения $\lambda_i$ матрицы S (по определению) выполняется
	$Sv_i=\lambda_iv_i$.

	Подставим выражение для $S$ и домножим слева на $B$:
	\[(B-\tau A)v_i=\lambda_iBv_i.\]

	Далее, умножим скалярно данное выражение на $v_i$ и выразим собственное
	значение:
	\[\big((B-\tau A)v_i, v_i\big)=\lambda_i(Bv_i,v_i)\quad\Leftrightarrow
	\quad\lambda_i=\frac{\big((B-\tau A)v_i, v_i\big)}{(Bv_i,v_i)}=
	1-\frac{\tau(Av_i, v_i)}{(Bv_i,v_i)}.\]

	Вспомним, что $B>\frac{1}{2}\tau A$, тогда, с одной стороны,
	\[\lambda_i>1-\frac{\tau(Av_i, v_i)}{0.5\tau(Av_i, v_i)}=-1.\]
	С другой же стороны, $\tau>0$, тогда
	\[\lambda_i<1.\]

	Мы показали, что $\forall i\; |\lambda_i|<1$, что по предыдущему
	критерию сходимости означает сходимость итерационного процесса.
\end{proof}

Теперь рассмотрим непосредственно сами итерационные процессы.
\subsubsection{Метод Якоби}
\begin{define}
	\textbf{Метод Якоби} -- итерационный метод
	\eqref{eq:sle_iterative_method}, в котором $\tau=1$, $B=D$, то есть
	\[B=
		\begin{pmatrix}
			a_{11}	& 0	& ...	& 0 \\
			0	& a_{22}& ...	& 0 \\
			\vdots	& \vdots&\ddots &\vdots \\
			0	& 0	& ...	& a_{nn} \\
		\end{pmatrix}
	\]
\end{define}

\begin{theorem}[о сходимости метода Якоби]
	Чтобы метод Якоби сходился, достаточно, чтобы матрица $A$
	обладала диагональным преобладанием
	\eqref{eq:diagonally_dominant_matrix}.
\end{theorem}

\begin{proof}
	Матрица $B=D$ очень легко обращается:
	\[B^{-1}=
		\begin{pmatrix}
			\frac{1}{a_{11}}	& 0	& ...	& 0 \\
			0	& \frac{1}{a_{22}}& ...	& 0 \\
			\vdots	& \vdots&\ddots &\vdots \\
			0	& 0	& ...	& \frac{1}{a_{nn}} \\
		\end{pmatrix}
	\]

	Соответственно, матрица перехода $S=E-B^{-1}A$ имеет вид
	\[S=
		\begin{pmatrix}
			0	&-\frac{a_{12}}{a_{11}} &... &
				-\frac{a_{1n}}{a_{11}} \\
			-\frac{a_{12}}{a_{22}}& 0	&... &
				-\frac{a_{2n}}{a_{22}} \\
			\vdots	& \vdots&\ddots &\vdots \\
			-\frac{a_{1n}}{a_{nn}}	& -\frac{a_{2n}}{a_{nn}} &
				...	& 0 \\
		\end{pmatrix}
	\]

	По условию, исходная матрица $A$ обладает диагональным преобладанием,
	поэтому её кубическая и октаэдрическая нормы меньше 1, что по
	теореме \eqref{eq:sle_matrix_norm_cond} достаточно для сходимости метода
	Якоби.
\end{proof}

\begin{example}\label{eq:jacobi_method_example}
	Решим систему
	\[
		\begin{pmatrix}
			5	& -3 \\
			-3	& 5  \\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			-28 \\
			36  \\
		\end{pmatrix}
		.
	\]
	итерационным методом Якоби. Сразу скажем, что точное решение этой
	системы -- $x_1=-2$, $x_2=6$.

	Матрица обладает диагональным преобладанием, чего достаточно для
	сходимости метода Якоби. Найдём матрицу перехода $S$:

	\[
		B=D=
		\begin{pmatrix}
			5	& 0 \\
			0	& 5  \\
		\end{pmatrix}
		\quad\Leftrightarrow\quad
		B^{-1}=
		\begin{pmatrix}
			0.2	& 0   \\
			0	& 0.2 \\
		\end{pmatrix}
		\quad\Leftrightarrow\quad
		S=E-B^{-1}A=
		\begin{pmatrix}
			0	& 0.6 \\
			0.6	& 0   \\
		\end{pmatrix}
		,
	\]

	Посчитаем ''добавку'':
	\[\tau B^{-1}f=
		\begin{pmatrix}
			-5.6 \\
			7.2  \\
		\end{pmatrix}
		.
	\]

	В качестве начального значения возьмём вектор $x^0=(1,1)^T$.

	Чтобы не записывать эти огромные матрицы, явно выразим компоненты
	вектора $x^m$ из формулы $x^{m+1}=Sx^m+\tau B^{-1}f$:
	\[x_1^{m+1}=0.6x_2^m-5.6,\quad x_2^{m+1}=0.6x_1^m+7.2.\]

	Составим таблицу, в которой отобразим компоненты итерационной
	последовательности $x^m$:

	\begin{tabular}{*{4}{|l}|}
		\hline
		$m$	& $x_1^m$	& $x_2^m$ & $\|z^m\|$ \\
		\hline
		0	& 1	& 1	& 5.831 \\
		\hline
		1	& -5.000 & 7.800& 3.500 \\
		\hline
		2	& -0.920 & 4.200& 2.099 \\
		\hline
		3	& -3.080 & 6.648& 1.259 \\
		\hline
		4	& -1.611 & 3.352& 0.756 \\
		\hline
		5	& -2.389 & 6.233& 0.453 \\
		\hline
		10	& -1.982 & 5.970& 0.035 \\
		\hline
		15	& -2.003 & 6.001& 0.002 \\
		\hline
	\end{tabular}
\end{example}

\subsubsection{Метод Зейделя}
\begin{define}
	\textbf{Метод Зейделя} -- итерационный метод
	\eqref{eq:sle_iterative_method}, в котором $\tau=1$, $B=L$, то есть
	\[B=
		\begin{pmatrix}
			a_{11}	& 0	& ...	& 0 \\
			a_{21}	& a_{22}& ...	& 0 \\
			\vdots	& \vdots&\ddots &\vdots \\
			a_{n1}	& a_{n2}& ...	& a_{nn} \\
		\end{pmatrix}
	\]
\end{define}

Для его сходимости может хватить того же условия, что и для метода Якоби.
\begin{theorem}[о сходимости метода Зейделя]
	Чтобы метод Зейделя сходился, достаточно, чтобы матрица $A$
	обладала диагональным преобладанием
	\eqref{eq:diagonally_dominant_matrix}.
\end{theorem}

\noproof

\begin{example}\label{eq:seidel_method_example}
	Решим систему из примера \eqref{eq:jacobi_method_example}
	\[
		\begin{pmatrix}
			5	& -3 \\
			-3	& 5  \\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			-28 \\
			36  \\
		\end{pmatrix}
	\]

	итерационным методом Зейделя. Как и прежде, точное решение этой
	системы -- $x_1=-2$, $x_2=6$.

	Матрица обладает диагональным преобладанием, чего достаточно для
	сходимости метода Зейделя. Найдём матрицу перехода $S$:

	\[
		B=L=
		\begin{pmatrix}
			5	& 0 \\
			-3	& 5  \\
		\end{pmatrix}
		\quad\Leftrightarrow\quad
		B^{-1}=
		\begin{pmatrix}
			0.2	& 0   \\
			0.12	& 0.2 \\
		\end{pmatrix}
		\quad\Leftrightarrow\quad
		S=
		\begin{pmatrix}
			0	& 0.6 \\
			0	& 0.36   \\
		\end{pmatrix}
		.
	\]

	Посчитаем ''добавку'':
	\[\tau B^{-1}f=
		\begin{pmatrix}
			-5.6 \\
			3.84 \\
		\end{pmatrix}
	\]

	В качестве начального значения возьмём вектор $x^0=(1,1)^T$.

	Чтобы не записывать эти огромные матрицы, явно выразим компоненты
	вектора $x^m$ из формулы $x^{m+1}=Sx^m+\tau B^{-1}f$:
	\[x_1^{m+1}=0.6x_2^m-5.6,\quad x_2^{m+1}=0.36x_2^m+3.84.\]

	Составим таблицу, в которой отобразим компоненты итерационной
	последовательности $x^m$:

	\begin{tabular}{*{4}{|l}|}
		\hline
		$m$	& $x_1^m$	& $x_2^m$ & $\|z^m\|$ \\
		\hline
		0	& 1	& 1	& 5.831 \\
		\hline
		1	& -5.000 & 4.200& 3.500 \\
		\hline
		2	& -3.080 & 5.352& 1.259 \\
		\hline
		3	& -2.389 & 5.767& 0.453 \\
		\hline
		4	& -2.140 & 5.916& 0.163 \\
		\hline
		5	& -2.050 & 5.970& 0.059 \\
		\hline
		10	& -2.0003&5.9998& 0.00036 \\
		\hline
	\end{tabular}\leavevmode\\

	Метод Зейделя сошёлся несколько ''быстрее'' метода Якоби.
\end{example}

\subsubsection{Метод простой итерации}
\begin{define}
	\textbf{Метод простой итерации} -- итерационный метод
	\eqref{eq:sle_iterative_method} вида
	\[x^{m+1}=x^m-\tau(Ax^m-f),\]
	то есть в качестве $B$ выбрана единичная матрица.
\end{define}

Да, тут параметр $\tau$ не задан, потому что его нужно ещё правильно задать.
Для этого есть следующая теорема.

\begin{theorem}[о сходимости метода простой итерации]
\label{eq:sle_fixed_point_convergence_cond}
	Чтобы метод простой итерации сходился, достаточно, чтобы матрица $A$
	была симметричной симметричной и положительно определёной, а
	итерационный параметр удовлетворял условию
	\[0<\tau<\frac{2}{\lambda_{max}},\qquad \lambda_{max}=\max\lambda(A).\]
\end{theorem}

\begin{proof}
	Поскольку матрица $A$ симметричная, то по лемме
	\eqref{eq:symmetric_matrix_props} все её собственные значения
	действительные, а из собственных векторов можно составить базис.

	Покажем, что все собственные значения матрицы $A$ положительные.
	По определению собственных векторов и значений,
	\[Av_i=\lambda_iv_i.\]

	Умножим скалярно на $v_i$ и выразим собственное значение:
	\[(Av_i,v_i)=\lambda_i(v_i,v_i)\quad\Leftrightarrow\quad
	\lambda_i=\frac{(Av_i,v_i)}{(v_i,v_i)}.\]

	Благодаря положительной определённости матрицы $A$ и скалярного
	квадрата, имеем, что все собственные значения матрицы положительные.

	Теперь, домножим собственный вектор $v_i$ слева на матрицу перехода
	$S=E-\tau A$:
	\[Sv_i=(E-\tau A)v_i=v_i-\tau\lambda_i v_i=(1-\tau\lambda_i)v_i.\]

	Таким образом, собственные векторы матрицы $A$ являются собственными и
	для матрицы $S$, а собственные значения связаны соотношением
	\[\lambda_i(S)=1-\tau\lambda_i.\]

	Для собственных значений матрицы $S$ по критерию
	\eqref{eq:sle_convergence_criterion} должно выполняться условие
	\[\forall i\in\overline{1,n}\;\;|\lambda_i(S)|<1.\]
	Если их перенести на собственные значения матрицы $A$, то получим
	условие
	\[\forall i\in\overline{1,n}\;\;0<\tau<\frac{2}{\lambda_i}\quad
	\Leftrightarrow\quad 0<\tau<\frac{2}{\lambda_{max}}\,\]
	что соответствует условию теоремы.
\end{proof}

\begin{example}
	И снова решим систему из примеров \eqref{eq:jacobi_method_example} и
	\eqref{eq:seidel_method_example}
	\[
		\begin{pmatrix}
			5	& -3 \\
			-3	& 5  \\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			-28 \\
			36  \\
		\end{pmatrix}
		,
	\]
	но теперь методом простой итерации. Матрица $A$ симметричная и
	положительно определённая (по критерию Сильвестра), поэтому мы можем
	установить границы для параметра $\tau$ по теореме
	\eqref{eq:sle_fixed_point_convergence_cond}.

	Несложно определить, что собственные значения матрицы $A$ равны
	$\lambda_1=2$, $\lambda_2=8$. Из них второе наибольшее, поэтому,
	чтобы метод сошёлся, достаточно $0<\tau<0.25$. Для определённости
	возьмём $\tau=0.2$.

	Найдём матрицу перехода $S$ и ''добавку'' $\tau B^{-1}f=\tau f$:
	\[S=E-0.2A=
		\begin{pmatrix}
			0	& 0.6 \\
			0.6	& 0   \\
		\end{pmatrix}
		,\qquad
		\tau f=
		\begin{pmatrix}
			-5.6 \\
			7.2  \\
		\end{pmatrix}
		.
	\]

	Заметим, что параметры итерационного процесса совпали с теми, что
	были получены в примере \eqref{eq:jacobi_method_example}, поэтому
	не станем повторять расчёты.
\end{example}

\end{document}
