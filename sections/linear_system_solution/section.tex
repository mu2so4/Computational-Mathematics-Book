\documentclass{article}

\begin{document}
\section{Вычислительные методы линейной алгебры}
Предмет исследования данного раздела -- это система линейных алгебраических
уравнений (СЛАУ)
\[
	\begin{cases}
		a_{11}x_1+a_{12}x_2+...+a_{1n}x_n=f_1, \\
		a_{21}x_1+a_{22}x_2+...+a_{2n}x_n=f_2, \\
		... \\
		a_{n1}x_1+a_{n2}x_2+...+a_{nn}x_n=f_n. \\
	\end{cases}
\]

Также можно записать систему в матричной форме как
\[Ax=f.\]

Если определитель матрицы $A$ отличен от нуля, то решение существует, и притом
только одно. Казалось, почему нельзя просто применить для решения метод Крамера
или обращение матрицы? Как и всегда: трудоёмкость этих методов быстро растёт с
ростом количества уравнений системы.

Например, для решения СЛАУ методом Крамера необходимо посчитать $n+1$
определитель матрицы, а вычислительная сложность этой операции составляет
$O(n!)$. В итоге, общая вычислительная мощность составляет примерно $O((n+1)!)$!
Для понимания масштабов трагедии: если система состоит из 20 уравнений, на
одну атомарную операцию уходит $10^{-12}$ секунд, и вычисления не
распараллеливаются и не векторизуются, то для решения потребуется 1.62 года
непрерывных вычислений, а также бесперебойной подачи электроэнергии.

А количество уравнений в системе можеть быть гораздо больше двадцати. Поэтому
большие системы решают иначе. В этом разделе будут рассмотрены методы Гаусса и
итерационные методы.

\newpage

\subsection{Линейные пространства}
Прежде чем как начать разбирать методы решения систем линейных уравнений,
освежим знания из линейной алгебры.

\begin{define}
	Пространство $L$ над полем $K$, в котором определены операции сложения
	$+: V^2\rightarrow V$ и умножения $*: K\times V\rightarrow V$,
	называется \textbf{линейным пространством}, если в нём верны следующие
	аксиомы:

	\begin{enumerate}[nosep]
		\item $\forall u,v\in V: u+v=u+v$ -- коммутативность сложения,
		\item $\forall u,v,w\in V: (u+v)+w=u+(v+w)$ -- ассоциативность
			сложения,
		\item $\exists\overline{0}: \forall v\in V:
			v+\overline{0}=\overline{0}+v=v$ -- существование
			нейтрального по сложению элемента,
		\item $\forall v\in V\;\exists (-v): v+(-v)=(-v)+v=\overline{0}$
			-- существование противоположного элемента,
		\item $\forall\lambda\in K,\;\forall u,v\in V: \lambda(u+v)=
			\lambda u+\lambda v$ -- дистрибутивность умножения
			относительно сложения векторов,
		\item $\forall\lambda,\mu\in K,\;\forall v\in V:
			(\lambda+\mu)v=\lambda v+\mu v$ -- дистрибутивность
			умножения,
		\item $\forall\lambda,\mu\in K,\;\forall v\in V: (\lambda\mu)v=
			\lambda(\mu v)$ -- ассоциативность умножения,
		\item $\exists 1\in K: \forall v\in V: 1\cdot v=v$ --
			существование нейтрального по умножению элемента.
	\end{enumerate}
\end{define}

При поиске решения СЛАУ полагаются $K=\mathbb R$ и $V=\mathbb R^n$.

\begin{define}\label{eq:dot_production}
	\textbf{Скалярным произведением} векторов из линейного пространства
	$\mathbb R^n$ называется отображение $\cdot: \mathbb R^n\times
	\mathbb R^n\rightarrow \mathbb R$, удовлетворяющее следующим условиям:
	\begin{enumerate}[nosep]
		\item $\forall v\in R^n: (v,v)\ge 0$, причём $(v,v)=0
			\Leftrightarrow v=\overline{0}$ -- положительная
			определённость;
		\item $\forall u,v\in \mathbb R^n: (u,v)=(v,u)$ --
			коммутативность;
		\item $\forall \lambda,\mu \in\mathbb R^n,\;
			\forall u,v,w\in \mathbb R^n: (\lambda u+\mu v, w)=
			\lambda(u,w)+\mu(v,w)$ -- линейность.
	\end{enumerate}
\end{define}
\newpage

Тут специально было приведено определение для вещественных векторов, потому что
над полем комплекных чисел коммутативность заменяется на сопряжённость. При
работе со СЛАУ векторы вещественные.

\begin{define}\label{eq:vector_norm}
	\textbf{Нормой вектора} из линейного пространства $V$ над полем $K$
	называется отображение $\|\|: V\rightarrow K$, удовлетворяющее
	следующим условиям:
	\begin{enumerate}[nosep]
		\item $\forall v\in V: \|v\|\ge 0$, причём $\|v\|=0
			\Leftrightarrow v=\overline{0}$;
		\item $\forall \lambda\in K,\;\forall v\in V:
			\|\lambda v\|=|\lambda|\cdot\|v\|$;
		\item $\forall u,v\in V: \|u+v\|\le\|u\|+\|v\|$ -- неравенство
			треугольника.
	\end{enumerate}
\end{define}

\begin{define}\label{eq:some_vector_norms}
	Определим следующие нормы над линейным пространством $\mathbb R^n$:
	\begin{enumerate}[nosep]
		\item $\mathlarger{\|x\|_2=\sqrt{(x,x)_2}=\sqrt{\sum_{i=1}^{n}
			x_i^2}}$ -- \textbf{Евклидова норма};
		\item $\mathlarger{\|x\|_1=\sum_{i=1}^{n}|x_i|}$ --
			\textbf{октаэдрическая норма}; \\
		\item $\mathlarger{\|x\|_\infty=\max_{i\in\overline{1,n}}
			|x_i|}$ -- \textbf{кубическая норма}.
	\end{enumerate}
\end{define}

Несложно проверить, что это в действительности нормы.

\subsubsection{Ортонормированные базисы}
\begin{define}
	\textbf{Базис} -- максимальный по включению упорядоченный набор линейно
	независимых векторов линейного пространства $V$.
\end{define}

Напомним важное свойство базиса пространства.
\begin{theorem}
	Всякий вектор $v$ из линейного пространства $V$ представим в виде
	линейной комбинации базисных векторов данного пространства, и притом
	единственным образом.
\end{theorem}

\begin{define}
	\textbf{Ортонормированный базис} линейного пространства $V$ над полем
	$K$ -- базис $e_1,...,e_n$ такой, что
	$\forall i,j\in\overline{1,k}: (e_i,e_j)=\delta_{ij}$.
\end{define}

\subsubsection{Матрицы}
\begin{define}
	Матрица $A$ \textbf{положительно определённая} над линейным
	пространством $\mathbb R^n$, если для всякого ненулевого вектора
	$v\in \mathbb R^n$ верно $(Av, v)_2>0$.
\end{define}

\begin{define}
	\textbf{Норма матрицы} $\mathbb R^{n\times m}$
	называется отображение $\|\|: \mathbb R^{n\times m}\rightarrow
	\mathbb R$, удовлетворяющее следующим условиям:
	\begin{enumerate}[nosep]
		\item $\forall A\in\mathbb R^{n\times m}: \|A\|\ge 0$, причём
			$\|A\|=0\Leftrightarrow A=O$;
		\item $\forall \lambda\in \mathbb R,\;\forall v\in
			\mathbb R^{n\times m}:
			\|\lambda A\|=|\lambda|\cdot\|A\|$;
		\item $\forall A,B\in\mathbb R^{n\times m}: \|A+B\|\le\|A\|+\|B\|$ --
			неравенство треугольника.
	\end{enumerate}
\end{define}

\begin{define}
	Норма $\|A\|$ матрицы $A\in \mathbb R^{n\times n}$ называется
	\textbf{нормой, подчинённой векторной норме} $\|x\|$
	\eqref{eq:vector_norm}, где $x\in \mathbb R^n$, если верно
	\[\|A\|=\max_{x\ne \overline{0}}\frac{\|Ax\|}{\|x\|}.\]
\end{define}

\begin{lemma}\label{eq:subordinate_norm_properties}
	Для всякой матричной нормы $\|A\|$, где $A\in\mathbb R^{n\times n}$,
	которая подчинена векторной норме $\|x\|$, где $x\in\mathbb R^n$, верны
	следующие неравенства:
	\begin{enumerate}[nosep]
		\item $\forall A\in\mathbb R^{n\times n},\;\forall x\in\mathbb
			R^n: \|Ax\|\le\|A\|\|x\|$;
		\item $\forall A,B\in\mathbb R^{n\times n}:
			\|AB\|\le\|A\|\|B\|$.
	\end{enumerate}
\end{lemma}

\begin{proof}
	Первое неравенство сразу доказывается из определения подчинённой нормы:
	\[\|A\|=\max_{x\ne\overline{0}}\frac{\|Ax\|}{\|x\|}\ge
	\frac{\|Ax\|}{\|x\|}\;\Leftrightarrow\;
	\|Ax\|\le\|A\|\|x\|.\]

	Второе доказывается чуть дольше:
	\[\|AB\|=\max_{x\ne\overline{0}}\frac{\|A(Bx)\|}{\|x\|}\le
	\max_{x\ne\overline{0}}\frac{\|A\|\|Bx\|}{\|x\|}=\|A\|\|B\|.\]
\end{proof}

\begin{lemma}
	Явные формы матричных норм, подчинённых октаэдрической и кубическим
	нормам \eqref{eq:some_vector_norms}, имеют вид
	\[\|A\|_1=\max_{j\in\overline{1,n}}\sum_{i=1}^{n}|a_{ij}|,\qquad
	\|A\|_\infty=\max_{i\in\overline{1,n}}\sum_{j=1}^{n}|a_{ij}|.\]
\end{lemma}

\proofexercise

В этом преимущество этих норм: подчинённые им матричные нормы вычисляются
непосредственно по элементам матрицы $A$, чего не скажешь о Евклидовой норме.

\begin{define}
	Скаляр $\lambda$ и ненулевой вектор $v$ называются \textbf{собственным
	значением} и \textbf{собственным вектором} матрицы $A$ соответственно,
	если для них выполняется равенство
	\[Av=\lambda v.\]
\end{define}

У собственных значений симметричных матриц есть одно замечательное свойство.
\begin{lemma}
	Если матрица $A$ симметричная, то все её собственные значения
	действительные.
\end{lemma}

\begin{theorem}[о Евклидовой норме симметричной матрицы]
\label{eq:symmetric_matrix_norm}
	Если матрица $A$ симметричная, то её Евклидова норма равна $\|A\|_2=
	\max|\lambda(A)|$, где $\lambda(A)$ -- спектр матрицы, или же множество
	её собственных значений.
\end{theorem}

\begin{corollary}
	Если матрица $A$ симметричная и не вырождена, то Евклидова норма
	обратной ей матрицы равна
	$\mathlarger{\|A^{-1}\|_2=\frac{1}{\min|\lambda(A)|}}$.
\end{corollary}

\proofexercise

\subsection{Система линейных алгебраических уравнений}
К решению систем линейных алгебраических уравнений сводятся практически все
задачи вычислительной математики.

Прежде чем переходить к изложению собственно методов, проведём краткое
исследование основных свойств самой системы линейных алгебраических уравнений. В
частности, чувствительности её решения к малым изменениям входных данных.

\begin{define}
	Будем считать, что задача поставлена \textbf{корректно}, если:
	\begin{enumerate}[nosep]
		\item Решение $x$ существует;
		\item Решение $x$ единственно;
		\item Решение $x$ непрерывно зависит от входных данных.
	\end{enumerate}
\end{define}

Если первые 2 пункта выполняются при $det(A)\ne 0$, так как тогда существует
обратная матрица, то третий пункт нужно рассмотреть поподробней.

\begin{theorem}[об оценке погрешности решения СЛАУ]
	Если $\delta f$ -- погрешность $f$ из системы линейных алгебраических
	уравнений $Ax=f$, матрица $A$ невырожденная, то погрешность решения
	$\delta x$ имеет оценку
	\[\boxed{\frac{\|\delta x\|}{\|x\|}\le\nu(A)\frac{\|\delta f\|}
	{\|f\|}},\]
	где $\nu(A)=\|A\|\|A^{-1}\|$ -- \textbf{число обусловленности}.
\end{theorem}

\begin{proof}
	Рассмотрим следующее равенство:
	\[A\delta x=\delta f.\]
	Поскольку матрица невырожденная, её можно обратить и перенести во вторую
	половину равенства:
	\[\delta x=A^{-1}\delta f.\]

	Равенство значений означает равенство норм. Воспользуемся оценкой по
	лемме \eqref{eq:subordinate_norm_properties} и получим
	\[\|\delta x\|\le\|A^{-1}\|\|\delta f\|.\]

	К исходному равенству $Ax=f$ применяем ту же оценку:
	\[\|f\|\le\|A\|\|x\|.\]

	Перемножим эти неравенства:
	\[\|\delta x\|\|f\|\le\|A\|\|A^{-1}\|\|x\|\|\delta f\|.\]

	Из него и следует оценка.
\end{proof}

Если $\nu(A)$ велико, то лишь очень малые погрешности входных данных гарантируют
малую погрешность решения. В связи с этим, про матрицы, у которых $\nu(A)$
велико, говорят, что они плохо обусловлены, а матрицы с относительно малой
величиной $\nu(A)$ называют хорошо обусловленными.

\begin{example}
	Рассмотрим матрицу
	$A=
		\begin{pmatrix}
			1	& 1 \\
			1	& 1+10^{-4} \\
		\end{pmatrix}
	$
	и систему линейных уравнений с ней
	\[
		\begin{pmatrix}
			1	& 1 \\
			1	& 1+10^{-4} \\
		\end{pmatrix}
		\begin{pmatrix}
			2 \\
			0 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			2 \\
			2 \\
		\end{pmatrix}
		.
	\]

	Теперь мы испортим систему, немного изменив $f$:
	\[f+\delta f=
		\begin{pmatrix}
			2 \\
			2+10^{-4} \\
		\end{pmatrix}
		\quad\Leftrightarrow\quad
		x+\delta x=
		\begin{pmatrix}
			1 \\
			1 \\
		\end{pmatrix}
	\]

	Совсем небольшое изменение условия привело к огромному изменению
	решения системы. Такую систему решать нельзя.

	Теперь найдём обусловленность матрицы $A$ по Евклидовой норме.
	Воспользуемся тем, что матрица симметричная
	\eqref{eq:symmetric_matrix_norm}, тогда $\|A\|_2\approx 2$.
	Норма же обратной матрицы, которая тоже симметричная,
	$\|A^{-1}\|_2\approx 2\cdot 10^4$. И тогда $\nu(A)\approx
	4\cdot 10^4$, то есть матрица плохо обусловленная. Результат был
	ожидаем.
\end{example}

\subsection{Методы Гаусса}
Метод Крамера далеко не единственный прямой метод решения системы линейных
алгебраических уравнений.

Метод Гаусса основан на том, чтобы элементарными преобразованиями привести
систему виду, который проще решать. Сюда относятся такие ''школьные'' методы
решения СЛАУ, как сложение уравнений и умножение их на ненулевой скаляр. Методы
Гаусса имеют вычислительную сложность $O(n^3)$ -- при такой сложности система
с 20 уравнениями решится за 8 нс, если применить условия из начала этого
раздела.

Сначала дадим некоторые виды ''хороших'' матриц.

\begin{define}
	Матрица $A$ называется \textbf{нижнетреугольной} и обозначается как $L$,
	если у неё нет ненулевых элементов \underline{выше} главной диагонали.
\end{define}

\begin{define}
	Матрица $A$ называется \textbf{верхнетреугольной} и обозначается как
	$U$, если у неё нет ненулевых элементов \underline{ниже} главной
	диагонали.
\end{define}

\begin{define}
	Матрица $A$ называется \textbf{диагональной} и обозначается как $D$,
	если все её ненулевые элементы расположены на главной диагонали.
\end{define}

Очевидно, что системы линейных уравнений, чья матрица имеет треугольный
или диагональный вид, очень легко решается.

\newpage

\begin{theorem}[об LU-разложении]
	Всякую невырожденную квадратную матрицу $A$ можно представить в виде
	произведения нижне- и верхнетреугольных матриц, то есть
	\[\forall A: det(A)\ne 0\quad\exists L,U: A=LU.\]
\end{theorem}

\subsubsection{Схема единственного деления}
\begin{define}
	\textbf{Угловой минор} матрицы $A$ порядка $k$ -- минор, который
	получается выделением первых $k$ строк и столбцов. Обозначается
	как $\Delta_k$.
\end{define}

\begin{algorithm}[метод Гаусса со схемой единственного деления]
\label{eq:gauss_single_division}
	Пусть необходимо решить уравение $Ax=f$. Допустим, что все угловые
	миноры матрицы $A:\Delta_i\ne 0$.

	На первом шаге первая строка матрицы делится на $\Delta_1=a_{11}$.
	Затем полученная строка вычитается из всех остальных так, чтобы занулить
	весь первый столбец ниже главной диагонали.

	На $k$-ом шаге $k$-я строка матрицы делится на
	$\frac{\Delta_k}{\Delta_{k-1}}$, а затем она вычитается из всех
	следующих уравнений так, чтобы занулить $k$ столбец ниже главной
	диагонали.

	После $n$ шагов мы получим верхнетреугольную матрицу, система с которой
	легко решается.
\end{algorithm}

\begin{example}
	Решим систему уравнений
	\[
		\begin{pmatrix}
			-6	& 1	& 1	\\
			3	& -1	& -2	\\
			1	& 3	& 3	\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
			x_3 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			-9 \\
			-1 \\
			11 \\
		\end{pmatrix}
	\]
	методом Гаусса со схемой единственного деления.

	Угловые миноры равны:
	\[\Delta_1=-6,\qquad\qquad\;\]
	\[\Delta_2=
		\begin{vmatrix}
			-6	& 1	\\
			3	& -1	\\
		\end{vmatrix}
		=3,
	\]
	\[\Delta_3=det(A)=-19.\]

	Разделим первое уравнение на $\Delta_1$ и вычтем его из второго и
	третьего так, чтобы занулить первый столбец ниже диагонали:
	\[
		\begin{pmatrix}
			1	&-\frac{1}{6}	& -\frac{1}{6}	\\
			3	& -1	& -2	\\
			1	& 3	& 3	\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
			x_3 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			\frac{3}{2} \\
			-1 \\
			11 \\
		\end{pmatrix}
		\quad\Leftrightarrow\quad
		\begin{pmatrix}
			1	&-\frac{1}{6}	& -\frac{1}{6}	\\
			0	& -\frac{1}{2}	& -\frac{3}{2}	\\
			0	& \frac{19}{6}	& \frac{19}{6}	\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
			x_3 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			\frac{3}{2} \\
			-\frac{11}{2} \\
			\frac{19}{2} \\
		\end{pmatrix}
	\]

	В получившейся системе, разделим второе уравнение на
	$\frac{\Delta_2}{\Delta_1}=-\frac{1}{2}$ и вычтем его так, чтобы
	занулить второй столбец ниже диагонали:
	\[
		\begin{pmatrix}
			1	&-\frac{1}{6}	& -\frac{1}{6}	\\
			0	& 1	& 3	\\
			0	& \frac{19}{6}	& \frac{19}{6}	\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
			x_3 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			\frac{3}{2} \\
			11 \\
			\frac{19}{2} \\
		\end{pmatrix}
		\quad\Leftrightarrow\quad
		\begin{pmatrix}
			1	&-\frac{1}{6}	& -\frac{1}{6}	\\
			0	& 1	& 3	\\
			0	& 0	& -\frac{19}{3}	\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
			x_3 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			\frac{3}{2} \\
			11 \\
			-\frac{76}{3} \\
		\end{pmatrix}
	\]

	Наконец, делим третье уравнение на $\frac{\Delta_2}{\Delta_1}=
	-\frac{19}{3}$:
	\[
		\begin{pmatrix}
			1	&-\frac{1}{6}	& -\frac{1}{6}	\\
			0	& 1	& 3	\\
			0	& 0	& 1	\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
			x_3 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			\frac{3}{2} \\
			11 \\
			4 \\
		\end{pmatrix}
	\]

	Получили верхнетреугольную матрицу. Вот окончательное решение системы:
	\[
		\begin{pmatrix}
			x_1 \\
			x_2 \\
			x_3 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			2 \\
			-1 \\
			4 \\
		\end{pmatrix}
	\]
\end{example}

\subsubsection{Метод в выбором главного элемента по столбцу}
Данный метод является улучшением предыдущего.

\begin{algorithm}[Метод Гаусса с выбором главного элемента по столбцу]
	Пусть необходимо решить уравнение $Ax=f$. Прежде чем как начать,
	поместим первой строкой то уравнение, которе имеет наибольший по модую
	коэффициент перед $x_1$.

	Дальнейшие шаги аналогичны предыдущему методу
	\eqref{eq:gauss_single_division}.
\end{algorithm}

Давайте посмотрим, что может произойти, если не сделать этот дополнительный шаг.

\begin{example}
	Решим систему уравнений
	\[
		\begin{pmatrix}
			10^{-8}	& 1	\\
			1	& 1	\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			1 \\
			2 \\
		\end{pmatrix}
	\]

	Умножим первое уравнение на $10^8$ и вычтем его из второго:
	\[
		\begin{pmatrix}
			1	& 10^8	\\
			1	& 1	\\
		\end{pmatrix}
		\begin{pmatrix}	
			x_1 \\
			x_2 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			10^8 \\
			2    \\
		\end{pmatrix}
		\quad\Leftrightarrow\quad
		\begin{pmatrix}
			1	& 10^8	\\
			0	&1-10^8	\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			10^8 \\
			2-10^8 \\
		\end{pmatrix}
	\]

	Разделим на $2-10^8$ второе уравнение и получим верхнетреугольную
	матрицу:
	\[
		\begin{pmatrix}
			1	& 10^8	\\
			0	& 1	\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			10^8 \\
			\frac{2-10^8}{1-10^8} \\
		\end{pmatrix}
	\]

	Округлив деление, мы получим, что
	\[
		\begin{pmatrix}
			x_1 \\
			x_2 \\
		\end{pmatrix}
		\approx
		\begin{pmatrix}
			0 \\
			1 \\
		\end{pmatrix}
	\]

	Но это даже близко не решение системы. Что пошло не так? Обусловленность
	матрицы $\nu_1(A)\approx 4$, что вполне неплохое значение. На деле, мы
	испортили обусловленность диагональной матрицы: $\nu_1(D_1)\approx
	10^{16}$, что является чудовищно огромным значением!

	Кстати, для справки: здесь погрешность вычислений меньше погрешности
	вещественного типа $float$.

	Попробум решить эту систему, но поменяем уравнения местами
	\[
		\begin{pmatrix}
			1	& 1	\\
			10^{-8}	& 1	\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			2 \\
			1 \\
		\end{pmatrix}
	\]

	Делить первое уравнение ни на что не надо, так что сразу вычтем его
	из второго:
	\[
		\begin{pmatrix}
			1	& 1	\\
			0	&1-10^{-8}\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			2 \\
			1-2\cdot 10^{-8} \\
		\end{pmatrix}
	\]

	И приведём систему к верхнетреугольному виду:
	\[
		\begin{pmatrix}
			1	& 1	\\
			0	& 1	\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			2 \\
			\frac{1-2\cdot 10^{-8}}{1-10^{-8}} \\
		\end{pmatrix}
	\]

	Округлив деление, мы получим, что
	\[
		\begin{pmatrix}
			x_1 \\
			x_2 \\
		\end{pmatrix}
		\approx
		\begin{pmatrix}
			1 \\
			1 \\
		\end{pmatrix}
	\]

	Это очень близко к точному значению. Что за магия обеспечила хорошую
	аппроксимацию решения? Всё просто: $\nu_1(D_2)=4$, то есть
	обусловленность получившейся верхнетреугольной матрицы практически не
	изменилась.
\end{example}

\subsection{Метод прогонки}
Трёхдиагональные матрицы, которые зачастую возникают из неявных разностных схем,
можно решить ещё проще; если точней, то за $O(n)$.

\begin{algorithm}[метод прогонки]
	Пусть необходимо решить уравнение $Ax=f$, где $A$ -- трёхдиагональная
	матрица. Обозначим элементы матрицы
	под главной диагональю $a_{i,i-1}=A_i$, при этом полагая $A_0=0$,
	элементы на диагонали $a_{i,i}=B_i$,
	и элементы над диагональю $a_{i,i+1}=C_i$, при этом полагая $C_n=0$,
	тогда матрица примет вид
	\[
		A=
		\begin{pmatrix}
			B_1	& C_1	& 0	& 0	& ...	& 0 \\
			A_2	& B_2	& C_2	& 0	& ...	& 0 \\
			0	& A_3	& B_3	& C_3	& ...	& \vdots\\
			\vdots	& \vdots& \vdots& \ddots& \ddots& C_{n-1} \\
			0	& 0	& 0	& ...	& A_n	& B_n\\
		\end{pmatrix}
	\]

	\textbf{Прямой ход}.
	Введём наборы коэффициентов $\alpha_i$ и $\beta_i$. На первом шагу
	\[\alpha_1=\frac{-C_1}{B_1},\quad \beta_1=\frac{f_1}{B_1},\]
	А на следующих шагах зададим их рекуррентно:
	\[\alpha_i=\frac{-C_i}{B_i+\alpha_{i-1}A_i},\quad
	\beta_i=\frac{f_i-\beta_{i-1}A_i}{B_i+\alpha_{i-1}A_i}.\]

	\textbf{Обратный ход}.
	Перейдём от этих коэффициентов непосредственно к решению системы:
	\[x_n=\beta_n,\]
	\[x_i=\alpha_i x_{i+1}+\beta_i,\quad i\in\overline{1,n-1}.\]

	Решение системы найдено.
\end{algorithm}

Теперь исследуем метод на устойчивость.

\begin{define}\label{eq:diagonally_dominant_matrix}
	Говорят, что квадратная матрица $A$ обладает \textbf{диагональным
	преобладанием}, если
	\[\forall i\in\overline{1,n}\quad |a_{ii}|>\sum_{j\ne i}|a_{ij}|.\]
\end{define}

\begin{theorem}
	Если для трёхдиагональной матрицы $A$ выполнено условие диагонального
	преобладания, причём
	\[0<\varepsilon<\min_{i\in\overline{1,n}}\Big(|a_{ii}|-
	\sum_{j\ne i}|a_{ij}|\Big),\]
	тогда для любого решения СЛАУ $Ax=f$ справедливо
	\[\boxed{\|x\|_\infty\le\frac{1}{\varepsilon}
	\|f\|_\infty}.\]
\end{theorem}

\begin{example}
	Решим систему линейных алгебраических уравнений
	\[
		\begin{pmatrix}
			-3	& 3	& 0	& 0	\\
			4	& 9	& 4	& 0	\\
			0	& -1	& 4	& 3	\\
			0	& 0	& -2	& 7	\\
		\end{pmatrix}
		\begin{pmatrix}
			x_1 \\
			x_2 \\
			x_3 \\
			x_4 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			24  \\
			-1  \\
			-14 \\
			-3  \\
		\end{pmatrix}
	\]

	Для этой матрицы выполнено условие диагонального преобладания, что
	гарантирует успешность использования метода прогонки.

	Считать мы будем немного иначе: мы будем от всех значений оставлять
	только до 3-х значащих цифр, а остальное округлять, чтобы сделать
	имитацию погрешности округлений в памяти компьютера.

	Прямой ход:

	\begin{tabular}{*{2}{l}}
		\vspace{4mm}
		$\mathlarger{\alpha_1=-\frac{3}{-3}=1}$, &
			$\mathlarger{\beta_1=\frac{24}{-3}=-8}$, \\
		\vspace{4mm}
		$\mathlarger{\alpha_2=\frac{-4}{9+1\cdot 4}}=-0.308$, &
			$\mathlarger{\beta_2=\frac{-1-(-8)\cdot 4}{9+1\cdot 4}=
			2.38}$, \\
		\vspace{4mm}
		$\mathlarger{\alpha_3=\frac{-3}{4+0.308\cdot (-1)}}=-0.813$, &
			$\mathlarger{\beta_3=\frac{-14-(2.38)\cdot (-1)}
			{4+0.308\cdot (-1)}=-3.15}$, \\
		$\mathlarger{\alpha_4=0}$, &
			$\mathlarger{\beta_4=x_4=\frac{-3-(-3.15)\cdot (-2)}
			{7+(-0.813)\cdot (-2)}=-1.08}$. \\
	\end{tabular}

	А теперь и обратный ход:
	\[x_3=(-0.813)\cdot (-1.08)+(-3.15)=-2.27,\]
	\[x_2=(-0.308)\cdot (-2.27)+2.38=3.08,\]
	\[x_1=1\cdot 3.08+(-8)=-4.92.\]

	Получились неплохие результаты, хотя последние коэффициенты накопили
	довольно значительную погрешность. Вот для сверки точное решение
	системы:
	\[
		\begin{pmatrix}
			x_1 \\
			x_2 \\
			x_3 \\
			x_4 \\
		\end{pmatrix}
		=
		\begin{pmatrix}
			-5 \\
			3 \\
			-2 \\
			-1 \\
		\end{pmatrix}
	\]
\end{example}

\end{document}
